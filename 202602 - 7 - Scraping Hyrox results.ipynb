{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4703437a-4581-443d-b88a-8fc5d12559b9",
   "metadata": {},
   "source": [
    "## üèãÔ∏è Scraping & Verifying HYROX Results Across Seasons\n",
    "\n",
    "**Competitions:** HYROX Seasons 1‚Äì8 (Official Results Portal)  \n",
    "**Purpose:** Scrape all available race results and verify that every race, division, and gender has been successfully collected  \n",
    "**Methods:** Selenium automation, controlled multi-threading, dynamic pagination handling, structured CSV export, division detection, empty-file tracking, and race-level completeness checks  \n",
    "**Author:** [Victoria Friss de Kereki](https://www.linkedin.com/in/victoria-friss-de-kereki/)  \n",
    "\n",
    "---\n",
    "\n",
    "**Notebook first written:** `23/02/2026`  \n",
    "**Last updated:** `27/02/2026`  \n",
    "\n",
    "> This notebook builds a **robust scraping and verification pipeline** for HYROX competition results.\n",
    "> \n",
    "> The workflow:\n",
    "> \n",
    "> - üåê Scrapes race results directly from the official HYROX results platform  \n",
    "> - üóÇ Organises outputs by **Season, Race, and Division**  \n",
    "> - üîÑ Handles dynamic page loading and pagination safely  \n",
    "> - üìÅ Saves structured CSV files for each race-division combination  \n",
    "> - ‚ö†Ô∏è Tracks empty divisions and failed scrapes  \n",
    "> - üèÅ Verifies that every race includes all the available divisions\n",
    "> \n",
    "> The objective of this notebook is to ensure **complete and reliable data extraction**, creating a solid foundation for downstream cleaning, validation, and analytical modelling in subsequent notebooks.\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ae46c-788b-442b-aec2-9f717e2392a2",
   "metadata": {},
   "source": [
    "### Scrape 50 at a time, for seasons and divisions available, even the empty ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f39576-26eb-408b-9598-590756dd655b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2026 Vienna] Starting scraping\n",
      "\n",
      "[2026 Turin] Starting scraping\n",
      "\n",
      "[2026 Washington DC] Starting scraping\n",
      "\n",
      "[2026 Katowice] Starting scraping\n",
      "\n",
      "[2026 Nice] Starting scraping\n",
      "\n",
      "[2026 Istanbul] Starting scraping\n",
      "\n",
      "[2026 Las Vegas] Starting scraping\n",
      "\n",
      "[2026 Bilbao] Starting scraping\n",
      "\n",
      "[2026 Phoenix] Starting scraping\n",
      "\n",
      "[2026 Osaka] Starting scraping\n",
      "\n",
      "[2025 Vancouver] Starting scraping\n",
      "\n",
      "[2026 Amsterdam] Starting scraping\n",
      "\n",
      "[2026 Auckland] Starting scraping\n",
      "\n",
      "[2025 Frankfurt] Starting scraping\n",
      "\n",
      "[2026 Fortaleza] Starting scraping\n",
      "\n",
      "[2025 Poznan] Starting scraping\n",
      "\n",
      "[2025 Gent] Starting scraping\n",
      "\n",
      "[2026 Taipei] Starting scraping\n",
      "\n",
      "[2026 Amsterdam - Youngstars] Starting scraping\n",
      "\n",
      "[2025 Shenzhen] Starting scraping\n",
      "[2026 Manchester] Starting scraping\n",
      "\n",
      "\n",
      "[2025 Anaheim] Starting scraping\n",
      "\n",
      "[2025 Stockholm] Starting scraping\n",
      "\n",
      "[2026 Guadalajara] Starting scraping\n",
      "\n",
      "[2025 Singapore Expo] Starting scraping\n",
      "\n",
      "[2025 Valencia] Starting scraping\n",
      "\n",
      "[2025 Madrid] Starting scraping\n",
      "\n",
      "[2025 Johannesburg] Starting scraping\n",
      "\n",
      "[2025 Boston] Starting scraping\n",
      "\n",
      "[2025 Mexico City] Starting scraping\n",
      "\n",
      "[2025 Seoul] Starting scraping\n",
      "\n",
      "[2026 St. Gallen] Starting scraping\n",
      "\n",
      "[2025 Bordeaux] Starting scraping\n",
      "\n",
      "[2025 Hamburg] Starting scraping\n",
      "\n",
      "[2025 Gdansk] Starting scraping\n",
      "\n",
      "[2025 Birmingham] Starting scraping\n",
      "\n",
      "[2025 Paris] Starting scraping\n",
      "\n",
      "[2025 Dallas] Starting scraping\n",
      "[2026 Vienna] All division CSVs exist. Skipping race.\n",
      "[2026 Bilbao] All division CSVs exist. Skipping race.\n",
      "[2026 Istanbul] All division CSVs exist. Skipping race.\n",
      "[2025 Vancouver] All division CSVs exist. Skipping race.\n",
      "[2026 Katowice] All division CSVs exist. Skipping race.\n",
      "[2026 Auckland] All division CSVs exist. Skipping race.\n",
      "[2026 Turin] All division CSVs exist. Skipping race.\n",
      "[2026 Fortaleza] All division CSVs exist. Skipping race.\n",
      "[2026 Washington DC] All division CSVs exist. Skipping race.\n",
      "[2026 Osaka] All division CSVs exist. Skipping race.\n",
      "[2026 Phoenix] All division CSVs exist. Skipping race.\n",
      "[2026 Amsterdam] All division CSVs exist. Skipping race.\n",
      "[2025 Poznan] All division CSVs exist. Skipping race.\n",
      "[2026 Las Vegas] All division CSVs exist. Skipping race.\n",
      "[2026 Nice] All division CSVs exist. Skipping race.\n",
      "[2025 Gent] All division CSVs exist. Skipping race.\n",
      "[2025 Frankfurt] All division CSVs exist. Skipping race.[2025 Anaheim] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Stockholm] All division CSVs exist. Skipping race.\n",
      "[2026 Guadalajara] All division CSVs exist. Skipping race.\n",
      "[2026 St. Gallen] All division CSVs exist. Skipping race.\n",
      "[2026 Taipei] All division CSVs exist. Skipping race.\n",
      "[2025 Shenzhen] All division CSVs exist. Skipping race.\n",
      "[2025 Valencia] All division CSVs exist. Skipping race.\n",
      "[2025 Singapore Expo] All division CSVs exist. Skipping race.\n",
      "[2025 Hamburg] All division CSVs exist. Skipping race.\n",
      "[2025 Paris] All division CSVs exist. Skipping race.\n",
      "[2026 Manchester] All division CSVs exist. Skipping race.\n",
      "[2025 Seoul] All division CSVs exist. Skipping race.\n",
      "[2025 Dallas] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Oslo] Starting scraping\n",
      "[2025 Boston] All division CSVs exist. Skipping race.\n",
      "[2025 Johannesburg] All division CSVs exist. Skipping race.\n",
      "[2025 Gdansk] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Atlanta] Starting scraping\n",
      "\n",
      "[2025 Delhi] Starting scraping\n",
      "\n",
      "[2025 Beijing] Starting scraping\n",
      "\n",
      "[2025 Rome] Starting scraping\n",
      "[2025 Madrid] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Mumbai] Starting scraping\n",
      "\n",
      "[2025 Cape Town] Starting scraping\n",
      "\n",
      "[2025 Maastricht] Starting scraping\n",
      "\n",
      "[2025 Hong Kong] Starting scraping\n",
      "[2025 Acapulco] Starting scraping\n",
      "\n",
      "[2025 Sao Paulo] Starting scraping\n",
      "\n",
      "[2025 Yokohama] Starting scraping\n",
      "\n",
      "[2025 Abu Dhabi] Starting scraping\n",
      "\n",
      "[2025 Sydney] Starting scraping\n",
      "\n",
      "[2025 Perth] Starting scraping\n",
      "[2025 Mexico City] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Birmingham] All division CSVs exist. Skipping race.\n",
      "[2025 Oslo] All division CSVs exist. Skipping race.\n",
      "[2025 Mumbai] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Singapore] Starting scraping\n",
      "[2025 Beijing] All division CSVs exist. Skipping race.\n",
      "[2025 Hong Kong] All division CSVs exist. Skipping race.\n",
      "[2025 Sydney] All division CSVs exist. Skipping race.\n",
      "[2025 Atlanta] All division CSVs exist. Skipping race.\n",
      "[2025 Maastricht] All division CSVs exist. Skipping race.\n",
      "[2025 Delhi] All division CSVs exist. Skipping race.\n",
      "[2025 Abu Dhabi] All division CSVs exist. Skipping race.\n",
      "[2025 Acapulco] All division CSVs exist. Skipping race.[2025 Perth] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Cape Town] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Melbourne] Starting scraping\n",
      "\n",
      "[2025 Utrecht] Starting scraping\n",
      "\n",
      "[2025 London Excel] Starting scraping\n",
      "\n",
      "[2025 Verona] Starting scraping\n",
      "[2026 Taipei] Skipped (all divisions exist)\n",
      "[2026 Fortaleza] Skipped (all divisions exist)\n",
      "[2026 Washington DC] Skipped (all divisions exist)\n",
      "[2026 Las Vegas] Skipped (all divisions exist)\n",
      "[2026 Katowice] Skipped (all divisions exist)\n",
      "[2026 Istanbul] Skipped (all divisions exist)\n",
      "[2026 Nice] Skipped (all divisions exist)\n",
      "[2026 Bilbao] Skipped (all divisions exist)\n",
      "[2026 Guadalajara] Skipped (all divisions exist)\n",
      "[2026 Vienna] Skipped (all divisions exist)\n",
      "[2026 Phoenix] Skipped (all divisions exist)\n",
      "[2026 Auckland] Skipped (all divisions exist)\n",
      "[2026 Osaka] Skipped (all divisions exist)\n",
      "[2026 Turin] Skipped (all divisions exist)\n",
      "[2026 Amsterdam] Skipped (all divisions exist)\n",
      "\n",
      "[2025 Dublin] Starting scraping\n",
      "\n",
      "[2025 Shanghai] Starting scraping\n",
      "\n",
      "[2025 Rio de Janeiro] Starting scraping\n",
      "[2025 Singapore] All division CSVs exist. Skipping race.\n",
      "[2025 London Excel] All division CSVs exist. Skipping race.\n",
      "[2025 Utrecht] All division CSVs exist. Skipping race.\n",
      "[2025 Melbourne] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Toronto] Starting scraping\n",
      "[2025 Rio de Janeiro] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Stuttgart] Starting scraping\n",
      "[2025 Shanghai] All division CSVs exist. Skipping race.\n",
      "[2025 Verona] All division CSVs exist. Skipping race.\n",
      "[2025 Dublin] All division CSVs exist. Skipping race.\n",
      "[2025 Toronto] All division CSVs exist. Skipping race.\n",
      "[2025 Stuttgart] All division CSVs exist. Skipping race.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {f\"Season_{i}\": f\"https://results.hyrox.com/season-{i}/\" for i in range(1, 9)}\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "MAX_THREADS = 50  # adjust how many to run at a time\n",
    "\n",
    "def human_pause(a=2, b=5):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# SELENIUM DRIVER\n",
    "# ==========================\n",
    "\n",
    "def create_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE PAGES FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(driver, race_name, division, gender_label, race_results):\n",
    "    is_doubles = \"DOUBLES\" in division.upper()\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(driver, 25).until(lambda d: \n",
    "                \"There are currently no results available\" in d.page_source\n",
    "                or len(d.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")) > 1\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"[{race_name}] Timeout waiting for {division} - {gender_label}\")\n",
    "            return False\n",
    "\n",
    "        # No results?\n",
    "        try:\n",
    "            no_result_elem = driver.find_element(By.XPATH,\n",
    "                \"//*[contains(text(),'There are currently no results available')]\"\n",
    "            )\n",
    "            if no_result_elem.is_displayed():\n",
    "                print(f\"[{race_name}] No results for {division} - {gender_label}\")\n",
    "                return False\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "        rows = [r for r in rows if \"list-group-header\" not in r.get_attribute(\"class\")]\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"[{race_name}] No rows found for {division} - {gender_label}\")\n",
    "            return False\n",
    "\n",
    "        scraped_any = False\n",
    "        for row in rows:\n",
    "            try:\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\", \"\").strip()\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\", \"\").strip()\n",
    "\n",
    "                if is_doubles:\n",
    "                    members = row.find_elements(By.CSS_SELECTOR, \".type-relay_member a\")\n",
    "                    member_names = \" & \".join([m.text for m in members])\n",
    "                    race_results.append([\n",
    "                        race_name, division, gender_label,\n",
    "                        rank, age_rank, member_names, \"\",\n",
    "                        age_group, total_time\n",
    "                    ])\n",
    "                else:\n",
    "                    name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "                    race_results.append([\n",
    "                        race_name, division, gender_label,\n",
    "                        rank, age_rank, name, nation,\n",
    "                        age_group, total_time\n",
    "                    ])\n",
    "                scraped_any = True\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"[{race_name}] Page {page_number} scraped for {division} - {gender_label}\")\n",
    "        page_number += 1\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            human_pause(2,5)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    return scraped_any\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE RACE FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_race(season, base_url, race_name, race_value):\n",
    "    driver = create_driver()\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "    print(f\"\\n[{race_name}] Starting scraping\")\n",
    "\n",
    "    driver.get(base_url)\n",
    "    human_pause(2,5)\n",
    "    Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "    human_pause(1,3)\n",
    "\n",
    "    # Get all divisions\n",
    "    select_division = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "    divisions = [o.text for o in select_division.options]\n",
    "\n",
    "    # Check if all division CSVs exist; if yes, skip the race entirely\n",
    "    all_exist = True\n",
    "    for div in divisions:\n",
    "        div_safe = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        file_path = os.path.join(season_folder, f\"{safe_name}_{div_safe}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            all_exist = False\n",
    "            break\n",
    "    if all_exist:\n",
    "        print(f\"[{race_name}] All division CSVs exist. Skipping race.\")\n",
    "        driver.quit()\n",
    "        return f\"[{race_name}] Skipped (all divisions exist)\"\n",
    "\n",
    "    for div in divisions:\n",
    "        div_safe = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        file_path = os.path.join(season_folder, f\"{safe_name}_{div_safe}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"[{race_name}] {div} already exists, will check for data\")\n",
    "        \n",
    "        print(f\"[{race_name}] Scraping division: {div}\")\n",
    "\n",
    "        driver.get(base_url)\n",
    "        human_pause(2,4)\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "        human_pause(1,2)\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "        human_pause(1,2)\n",
    "\n",
    "        try:\n",
    "            gender_dropdown = Select(driver.find_element(By.ID, \"default-lists-sex\"))\n",
    "            genders = [(o.get_attribute(\"value\"), o.text) for o in gender_dropdown.options]\n",
    "        except NoSuchElementException:\n",
    "            genders = [(\"\", \"All\")]\n",
    "\n",
    "        division_results = []\n",
    "        division_has_data = False\n",
    "\n",
    "        for gender_code, gender_label in genders:\n",
    "            print(f\"[{race_name}] Scraping gender: {gender_label} in {div}\")\n",
    "\n",
    "            driver.get(base_url)\n",
    "            human_pause(2,4)\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "            human_pause(1,2)\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "            human_pause(1,2)\n",
    "            if gender_code:\n",
    "                Select(driver.find_element(By.ID, \"default-lists-sex\")).select_by_value(gender_code)\n",
    "            Select(driver.find_element(By.ID, \"default-num_results\")).select_by_value(\"100\")\n",
    "            human_pause(1,2)\n",
    "            driver.find_element(By.ID, \"default-submit\").click()\n",
    "            human_pause(2,4)\n",
    "\n",
    "            has_data = scrape_pages(driver, race_name, div, gender_label, division_results)\n",
    "            if has_data:\n",
    "                division_has_data = True\n",
    "\n",
    "        # Save CSV even if empty (no data)\n",
    "        df = pd.DataFrame(\n",
    "            division_results,\n",
    "            columns=[\"Race\",\"Division\",\"Gender\",\n",
    "                     \"Rank Overall\",\"Rank Age Group\",\n",
    "                     \"Name\",\"Nation\",\"Age Group\",\"Total Time\"]\n",
    "        )\n",
    "        df.to_csv(file_path, index=False)\n",
    "        if division_has_data:\n",
    "            print(f\"[{race_name}] Saved {len(df)} rows for {div}\")\n",
    "        else:\n",
    "            print(f\"[{race_name}] Division {div} had no data. Empty CSV saved.\")\n",
    "\n",
    "    driver.quit()\n",
    "    return f\"[{race_name}] Finished scraping\"\n",
    "\n",
    "# ==========================\n",
    "# MAIN EXECUTION\n",
    "# ==========================\n",
    "\n",
    "all_tasks = []\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "    driver_main = create_driver()\n",
    "    driver_main.get(base_url)\n",
    "    human_pause(2,4)\n",
    "    try:\n",
    "        WebDriverWait(driver_main, 20).until(EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\")))\n",
    "    except TimeoutException:\n",
    "        driver_main.quit()\n",
    "        continue\n",
    "\n",
    "    race_dropdown = Select(driver_main.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "    races = [(race_dropdown.options[i].text, race_dropdown.options[i].get_attribute(\"value\")) for i in range(len(race_dropdown.options))]\n",
    "    driver_main.quit()\n",
    "\n",
    "    for race_name, race_value in races:\n",
    "        all_tasks.append((season, base_url, race_name, race_value))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    futures = [executor.submit(scrape_race, *task) for task in all_tasks]\n",
    "    for f in futures:\n",
    "        print(f.result())\n",
    "\n",
    "print(\"\\nALL DONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7a28b-16b1-4213-9b5f-691136e63913",
   "metadata": {},
   "source": [
    "### Check all available seasons/races/divisions have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fbea2-3fc1-4a36-97d7-c16a9f12e03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "VERIFYING HYROX DATASET\n",
      "==============================\n",
      "\n",
      "========== CHECKING Season_1 ==========\n",
      "Checking race: World Championships\n",
      "Checking race: 2019 Oberhausen\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {f\"Season_{i}\": f\"https://results.hyrox.com/season-{i}/\" for i in range(1, 9)}\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "\n",
    "# ==========================\n",
    "# DRIVER\n",
    "# ==========================\n",
    "\n",
    "def create_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ==========================\n",
    "# VERIFICATION\n",
    "# ==========================\n",
    "\n",
    "def verify_all_downloads():\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"VERIFYING HYROX DATASET\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    for season, base_url in BASE_URLS.items():\n",
    "\n",
    "        print(f\"\\n========== CHECKING {season} ==========\")\n",
    "\n",
    "        season_folder = os.path.join(SAVE_ROOT, season)\n",
    "\n",
    "        if not os.path.exists(season_folder):\n",
    "            print(f\"‚ùå Season folder missing: {season}\")\n",
    "            continue\n",
    "\n",
    "        local_files = set(os.listdir(season_folder))\n",
    "\n",
    "        driver = create_driver()\n",
    "        driver.get(base_url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                \n",
    "                EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"‚ùå Could not load season page: {season}\")\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "        race_dropdown = Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "        races = [(opt.text.strip(), opt.get_attribute(\"value\"))\n",
    "                 for opt in race_dropdown.options]\n",
    "\n",
    "        missing_races = []\n",
    "        missing_divisions = []\n",
    "\n",
    "        for race_name, race_value in races:\n",
    "\n",
    "            safe_race = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "            print(f\"Checking race: {race_name}\")\n",
    "\n",
    "            driver.get(base_url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\\\n",
    "                .select_by_value(race_value)\n",
    "            time.sleep(2)\n",
    "\n",
    "            division_dropdown = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "            divisions = [opt.text.strip() for opt in division_dropdown.options]\n",
    "\n",
    "            race_files = [f for f in local_files if f.startswith(safe_race + \"_\")]\n",
    "\n",
    "            if not race_files:\n",
    "                missing_races.append(race_name)\n",
    "\n",
    "            race_missing_divs = []\n",
    "\n",
    "            for div in divisions:\n",
    "                safe_div = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "                expected_filename = f\"{safe_race}_{safe_div}.csv\"\n",
    "\n",
    "                if expected_filename not in local_files:\n",
    "                    race_missing_divs.append(div)\n",
    "\n",
    "            if race_missing_divs:\n",
    "                missing_divisions.append((race_name, race_missing_divs))\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # REPORT\n",
    "        if not missing_races:\n",
    "            print(\"‚úÖ No missing races.\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Missing races:\")\n",
    "            for r in missing_races:\n",
    "                print(f\"   - {r}\")\n",
    "\n",
    "        if not missing_divisions:\n",
    "            print(\"‚úÖ All divisions present.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö† Missing divisions:\")\n",
    "            for race, divs in missing_divisions:\n",
    "                print(f\"\\n  {race}\")\n",
    "                for d in divs:\n",
    "                    print(f\"     - {d}\")\n",
    "\n",
    "        print(f\"\\n========== DONE {season} ==========\")\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"VERIFICATION COMPLETE\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "# ==========================\n",
    "# RUN\n",
    "# ==========================\n",
    "\n",
    "verify_all_downloads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f3f02-b922-49c4-8e37-3314ae1fe9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
