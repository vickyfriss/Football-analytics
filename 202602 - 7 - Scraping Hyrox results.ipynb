{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266ae46c-788b-442b-aec2-9f717e2392a2",
   "metadata": {},
   "source": [
    "# Scrape 50 at a time, for all divisions available for all seasons available, even the empty ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f39576-26eb-408b-9598-590756dd655b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2026 Las Vegas] Starting scraping\n",
      "\n",
      "[2025 Verona] Starting scraping\n",
      "\n",
      "[2025 Dallas] Starting scraping\n",
      "\n",
      "[2025 Bordeaux] Starting scraping\n",
      "\n",
      "[2025 Chicago] Starting scraping\n",
      "\n",
      "[2025 Atlanta] Starting scraping\n",
      "\n",
      "[2025 Singapore Expo] Starting scraping\n",
      "[2025 Mexico City] Starting scraping\n",
      "\n",
      "\n",
      "[2025 Rio de Janeiro] Starting scraping\n",
      "\n",
      "[2025 Seoul] Starting scraping\n",
      "\n",
      "[2025 Dublin] Starting scraping\n",
      "\n",
      "[2025 Melbourne] Starting scraping\n",
      "\n",
      "[2025 Utrecht] Starting scraping\n",
      "\n",
      "[2025 Madrid] Starting scraping\n",
      "\n",
      "[2026 Taipei] Starting scraping\n",
      "\n",
      "[2025 Shanghai] Starting scraping\n",
      "\n",
      "[2026 Amsterdam] Starting scraping\n",
      "\n",
      "[2026 Auckland] Starting scraping\n",
      "\n",
      "[2026 Bilbao] Starting scraping\n",
      "[2025 Anaheim] Starting scraping\n",
      "\n",
      "[2026 Turin] Starting scraping\n",
      "\n",
      "[2025 Johannesburg] Starting scraping\n",
      "\n",
      "[2025 Frankfurt] Starting scraping\n",
      "\n",
      "[2026 Guadalajara] Starting scraping\n",
      "\n",
      "[2025 Shenzhen] Starting scraping\n",
      "\n",
      "[2026 Amsterdam - Youngstars] Starting scraping\n",
      "\n",
      "[2026 Washington DC] Starting scraping\n",
      "\n",
      "[2026 Nice] Starting scraping\n",
      "\n",
      "[2026 Vienna] Starting scraping\n",
      "\n",
      "[2026 Katowice] Starting scraping\n",
      "\n",
      "[2026 Manchester] Starting scraping\n",
      "\n",
      "[2026 St. Gallen] Starting scraping\n",
      "\n",
      "[2026 Phoenix] Starting scraping\n",
      "\n",
      "[2025 Stockholm] Starting scraping\n",
      "\n",
      "[2026 Istanbul] Starting scraping\n",
      "\n",
      "\n",
      "[2025 London Excel] Starting scraping\n",
      "\n",
      "[2025 Poznan] Starting scraping\n",
      "[2025 Atlanta] Scraping division: HYROX PRO - Sunday\n",
      "[2025 Rio de Janeiro] All division CSVs exist. Skipping race.\n",
      "[2025 Singapore Expo] All division CSVs exist. Skipping race.\n",
      "[2026 Guadalajara] HYROX PRO already exists, will check for data\n",
      "[2026 Guadalajara] Scraping division: HYROX PRO\n",
      "[2026 Taipei] All division CSVs exist. Skipping race.\n",
      "[2025 Dallas] All division CSVs exist. Skipping race.\n",
      "[2025 Shenzhen] All division CSVs exist. Skipping race.\n",
      "[2025 Verona] HYROX PRO - Sunday already exists, will check for data\n",
      "[2025 Verona] Scraping division: HYROX PRO - Sunday\n",
      "[2026 Las Vegas] All division CSVs exist. Skipping race.\n",
      "[2025 Bordeaux] All division CSVs exist. Skipping race.\n",
      "[2025 Seoul] All division CSVs exist. Skipping race.\n",
      "[2025 Shanghai] All division CSVs exist. Skipping race.\n",
      "[2026 Bilbao] HYROX PRO already exists, will check for data\n",
      "[2026 Bilbao] Scraping division: HYROX PRO\n",
      "[2025 Melbourne] HYROX ELITE 15 - Friday already exists, will check for data\n",
      "[2025 Melbourne] Scraping division: HYROX ELITE 15 - Friday\n",
      "[2026 Auckland] All division CSVs exist. Skipping race.\n",
      "[2025 Chicago] All division CSVs exist. Skipping race.\n",
      "[2025 Utrecht] All division CSVs exist. Skipping race.\n",
      "[2025 Johannesburg] HYROX PRO already exists, will check for data\n",
      "[2025 Johannesburg] Scraping division: HYROX PRO\n",
      "[2026 Washington DC] All division CSVs exist. Skipping race.\n",
      "[2025 Mexico City] All division CSVs exist. Skipping race.\n",
      "[2025 Dublin] All division CSVs exist. Skipping race.\n",
      "[2026 Amsterdam] All division CSVs exist. Skipping race.\n",
      "[2026 St. Gallen] HYROX PRO already exists, will check for data\n",
      "[2026 St. Gallen] Scraping division: HYROX PRO\n",
      "[2025 Stockholm] All division CSVs exist. Skipping race.\n",
      "[2026 Katowice] All division CSVs exist. Skipping race.\n",
      "[2025 Madrid] All division CSVs exist. Skipping race.\n",
      "[2025 Verona] Scraping gender: Men in HYROX PRO - Sunday[2026 Guadalajara] Scraping gender: Men in HYROX PRO\n",
      "\n",
      "[2025 Gent] Starting scraping\n",
      "\n",
      "[2026 Fortaleza] Starting scraping\n",
      "\n",
      "[2026 Bilbao] Scraping gender: Men in HYROX PRO\n",
      "[2025 Melbourne] Scraping gender: Men in HYROX ELITE 15 - Friday\n",
      "[2025 Atlanta] Scraping gender: Men in HYROX PRO - Sunday\n",
      "[2025 Frankfurt] HYROX PRO already exists, will check for data\n",
      "[2025 Frankfurt] Scraping division: HYROX PRO\n",
      "[2026 Manchester] All division CSVs exist. Skipping race.\n",
      "[2026 St. Gallen] Scraping gender: Men in HYROX PRO\n",
      "\n",
      "[2025 Vancouver] Starting scraping\n",
      "[2026 Amsterdam - Youngstars] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2026 Osaka] Starting scraping\n",
      "[2025 Johannesburg] Scraping gender: Men in HYROX PRO\n",
      "[2026 Vienna] All division CSVs exist. Skipping race.\n",
      "[2026 Istanbul] Scraping division: HYROX PRO - Saturday\n",
      "[2026 Phoenix] All division CSVs exist. Skipping race.\n",
      "[2025 Poznan] All division CSVs exist. Skipping race.\n",
      "[2025 Anaheim] All division CSVs exist. Skipping race.\n",
      "[2025 Frankfurt] Scraping gender: Men in HYROX PRO\n",
      "[2025 Gent] HYROX PRO - Sunday already exists, will check for data\n",
      "[2025 Gent] Scraping division: HYROX PRO - Sunday\n",
      "[2025 Melbourne] Page 1 scraped for HYROX ELITE 15 - Friday - Men\n",
      "[2025 Melbourne] Scraping gender: Women in HYROX ELITE 15 - Friday\n",
      "[2025 Gent] Scraping gender: Men in HYROX PRO - Sunday\n",
      "[2026 Osaka] All division CSVs exist. Skipping race.\n",
      "\n",
      "[2025 Oslo] Starting scraping\n",
      "[2026 Fortaleza] All division CSVs exist. Skipping race.\n",
      "[2025 Vancouver] All division CSVs exist. Skipping race.\n",
      "[2025 Melbourne] Page 1 scraped for HYROX ELITE 15 - Friday - Women\n",
      "\n",
      "[2025 Rome] Starting scraping\n",
      "[2026 Guadalajara] Page 1 scraped for HYROX PRO - Men\n",
      "[2025 Verona] Page 1 scraped for HYROX PRO - Sunday - Men\n",
      "[2026 Bilbao] Page 1 scraped for HYROX PRO - Men\n",
      "[2026 St. Gallen] Page 1 scraped for HYROX PRO - Men\n",
      "[2025 Atlanta] Page 1 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Oslo] All division CSVs exist. Skipping race.\n",
      "[2026 St. Gallen] Page 2 scraped for HYROX PRO - Men\n",
      "[2026 St. Gallen] Scraping gender: Women in HYROX PRO\n",
      "[2025 Johannesburg] Page 1 scraped for HYROX PRO - Men\n",
      "[2025 Frankfurt] Page 1 scraped for HYROX PRO - Men\n",
      "[2025 Melbourne] Division HYROX ELITE 15 - Friday had no data. Empty CSV saved.\n",
      "\n",
      "[2025 Stuttgart] Starting scraping\n",
      "[2025 Melbourne] HYROX ELITE 15 DOUBLES - Saturday already exists, will check for data\n",
      "[2025 Melbourne] Scraping division: HYROX ELITE 15 DOUBLES - Saturday\n",
      "[2025 Johannesburg] Page 2 scraped for HYROX PRO - Men\n",
      "[2025 Johannesburg] Scraping gender: Women in HYROX PRO\n",
      "[2025 Melbourne] Scraping gender: Men in HYROX ELITE 15 DOUBLES - Saturday\n",
      "[2025 Rome] All division CSVs exist. Skipping race.\n",
      "[2025 Gent] Page 1 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Atlanta] Page 2 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Atlanta] Scraping gender: Women in HYROX PRO - Sunday\n",
      "[2025 Melbourne] Page 1 scraped for HYROX ELITE 15 DOUBLES - Saturday - Men\n",
      "[2025 Melbourne] Scraping gender: Women in HYROX ELITE 15 DOUBLES - Saturday\n",
      "[2026 Guadalajara] Page 2 scraped for HYROX PRO - Men\n",
      "[2026 Bilbao] Page 2 scraped for HYROX PRO - Men\n",
      "[2025 Verona] Page 2 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Stuttgart] HYROX PRO - Friday already exists, will check for data\n",
      "[2025 Stuttgart] Scraping division: HYROX PRO - Friday\n",
      "[2026 Guadalajara] Page 3 scraped for HYROX PRO - Men\n",
      "[2026 Guadalajara] Scraping gender: Women in HYROX PRO\n",
      "[2025 Stuttgart] Scraping gender: Men in HYROX PRO - Friday\n",
      "[2025 Melbourne] Page 1 scraped for HYROX ELITE 15 DOUBLES - Saturday - Women\n",
      "[2026 Taipei] Skipped (all divisions exist)\n",
      "[2026 Fortaleza] Skipped (all divisions exist)\n",
      "[2026 Washington DC] Skipped (all divisions exist)\n",
      "[2026 Las Vegas] Skipped (all divisions exist)\n",
      "[2026 Katowice] Skipped (all divisions exist)\n",
      "[2025 Melbourne] Division HYROX ELITE 15 DOUBLES - Saturday had no data. Empty CSV saved.\n",
      "[2025 Melbourne] HYROX PRO - Saturday already exists, will check for data\n",
      "[2025 Melbourne] Scraping division: HYROX PRO - Saturday\n",
      "\n",
      "[2025 Abu Dhabi] Starting scraping\n",
      "\n",
      "[2025 Beijing] Starting scraping\n",
      "\n",
      "[2025 Gdansk] Starting scraping\n",
      "\n",
      "[2025 Sydney] Starting scraping\n",
      "\n",
      "[2025 Delhi] Starting scraping\n",
      "\n",
      "[2025 Boston] Starting scraping\n",
      "\n",
      "[2025 Cape Town] Starting scraping\n",
      "\n",
      "[2025 Valencia] Starting scraping\n",
      "\n",
      "[2025 Singapore] Starting scraping\n",
      "\n",
      "[2025 Geneva] Starting scraping\n",
      "\n",
      "[2025 Hong Kong] Starting scraping\n",
      "[2025 Melbourne] Scraping gender: Men in HYROX PRO - Saturday\n",
      "[2025 Delhi] All division CSVs exist. Skipping race.\n",
      "[2025 Beijing] All division CSVs exist. Skipping race.\n",
      "[2025 Singapore] All division CSVs exist. Skipping race.\n",
      "[2025 Gdansk] All division CSVs exist. Skipping race.\n",
      "[2025 Geneva] All division CSVs exist. Skipping race.\n",
      "[2025 Cape Town] All division CSVs exist. Skipping race.\n",
      "[2025 Boston] All division CSVs exist. Skipping race.\n",
      "[2025 Valencia] All division CSVs exist. Skipping race.\n",
      "[2025 Abu Dhabi] All division CSVs exist. Skipping race.\n",
      "[2025 Hong Kong] All division CSVs exist. Skipping race.\n",
      "[2026 St. Gallen] Page 1 scraped for HYROX PRO - Women\n",
      "[2026 St. Gallen] Saved 219 rows for HYROX PRO\n",
      "[2026 St. Gallen] HYROX already exists, will check for data\n",
      "[2026 St. Gallen] Scraping division: HYROX\n",
      "[2025 Atlanta] Page 1 scraped for HYROX PRO - Sunday - Women\n",
      "[2025 Atlanta] Saved 252 rows for HYROX PRO - Sunday\n",
      "[2025 Atlanta] Scraping division: HYROX - Saturday\n",
      "[2026 St. Gallen] Scraping gender: Men in HYROX\n",
      "[2025 Johannesburg] Page 1 scraped for HYROX PRO - Women\n",
      "[2025 Johannesburg] Saved 175 rows for HYROX PRO\n",
      "[2025 Johannesburg] Scraping division: HYROX\n",
      "[2025 Atlanta] Scraping gender: Men in HYROX - Saturday\n",
      "[2025 Johannesburg] Scraping gender: Men in HYROX\n",
      "[2025 Frankfurt] Page 2 scraped for HYROX PRO - Men\n",
      "[2025 Verona] Page 3 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Verona] Scraping gender: Women in HYROX PRO - Sunday\n",
      "[2026 Bilbao] Page 3 scraped for HYROX PRO - Men\n",
      "[2026 Bilbao] Scraping gender: Women in HYROX PRO\n",
      "[2026 Guadalajara] Page 1 scraped for HYROX PRO - Women\n",
      "[2025 Gent] Page 2 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Stuttgart] Page 1 scraped for HYROX PRO - Friday - Men\n",
      "[2026 Guadalajara] Page 2 scraped for HYROX PRO - Women\n",
      "[2026 Guadalajara] Saved 318 rows for HYROX PRO\n",
      "[2026 Guadalajara] HYROX already exists, will check for data\n",
      "[2026 Guadalajara] Scraping division: HYROX\n",
      "[2025 Melbourne] Page 1 scraped for HYROX PRO - Saturday - Men\n",
      "[2026 Guadalajara] Scraping gender: Men in HYROX\n",
      "[2025 Frankfurt] Page 3 scraped for HYROX PRO - Men\n",
      "[2025 Frankfurt] Scraping gender: Women in HYROX PRO\n",
      "[2026 St. Gallen] Page 1 scraped for HYROX - Men\n",
      "[2025 Verona] Page 1 scraped for HYROX PRO - Sunday - Women\n",
      "[2025 Atlanta] Page 1 scraped for HYROX - Saturday - Men\n",
      "[2026 Bilbao] Page 1 scraped for HYROX PRO - Women\n",
      "[2025 Stuttgart] Page 2 scraped for HYROX PRO - Friday - Men\n",
      "[2026 Bilbao] Page 2 scraped for HYROX PRO - Women\n",
      "[2025 Johannesburg] Page 1 scraped for HYROX - Men\n",
      "[2025 Melbourne] Page 2 scraped for HYROX PRO - Saturday - Men\n",
      "[2026 Bilbao] Saved 388 rows for HYROX PRO\n",
      "[2026 Bilbao] Scraping division: HYROX\n",
      "[2025 Verona] Page 2 scraped for HYROX PRO - Sunday - Women\n",
      "[2025 Verona] Saved 414 rows for HYROX PRO - Sunday\n",
      "[2025 Verona] HYROX - Friday already exists, will check for data\n",
      "[2025 Verona] Scraping division: HYROX - Friday\n",
      "[2026 Guadalajara] Page 1 scraped for HYROX - Men\n",
      "[2025 Gent] Page 3 scraped for HYROX PRO - Sunday - Men\n",
      "[2026 Bilbao] Scraping gender: Men in HYROX\n",
      "[2026 St. Gallen] Page 2 scraped for HYROX - Men\n",
      "[2025 Verona] Scraping gender: Men in HYROX - Friday\n",
      "[2025 Gent] Page 4 scraped for HYROX PRO - Sunday - Men\n",
      "[2025 Gent] Scraping gender: Women in HYROX PRO - Sunday\n",
      "[2025 Frankfurt] Page 1 scraped for HYROX PRO - Women\n",
      "[2025 Atlanta] Page 2 scraped for HYROX - Saturday - Men\n",
      "[2025 Stuttgart] Page 3 scraped for HYROX PRO - Friday - Men\n",
      "[2025 Melbourne] Page 3 scraped for HYROX PRO - Saturday - Men\n",
      "[2026 Guadalajara] Page 2 scraped for HYROX - Men\n",
      "[2026 St. Gallen] Page 3 scraped for HYROX - Men\n",
      "[2026 Bilbao] Page 1 scraped for HYROX - Men\n",
      "[2025 Verona] Page 1 scraped for HYROX - Friday - Men\n",
      "[2025 Stuttgart] Page 4 scraped for HYROX PRO - Friday - Men\n",
      "[2025 Stuttgart] Scraping gender: Women in HYROX PRO - Friday\n",
      "[2025 Johannesburg] Page 2 scraped for HYROX - Men\n",
      "[2025 Frankfurt] Page 2 scraped for HYROX PRO - Women\n",
      "[2025 Frankfurt] Saved 428 rows for HYROX PRO\n",
      "[2025 Frankfurt] HYROX already exists, will check for data\n",
      "[2025 Frankfurt] Scraping division: HYROX\n",
      "[2025 Atlanta] Page 3 scraped for HYROX - Saturday - Men\n",
      "[2025 Frankfurt] Scraping gender: Men in HYROX\n",
      "[2025 Gent] Page 1 scraped for HYROX PRO - Sunday - Women\n",
      "[2025 Melbourne] Page 4 scraped for HYROX PRO - Saturday - Men\n",
      "[2026 Guadalajara] Page 3 scraped for HYROX - Men\n",
      "[2026 St. Gallen] Page 4 scraped for HYROX - Men\n",
      "[2025 Gent] Page 2 scraped for HYROX PRO - Sunday - Women\n",
      "[2026 Bilbao] Page 2 scraped for HYROX - Men\n",
      "[2025 Gent] Saved 420 rows for HYROX PRO - Sunday\n",
      "[2025 Gent] HYROX - Saturday already exists, will check for data\n",
      "[2025 Gent] Scraping division: HYROX - Saturday\n",
      "[2025 Gent] Scraping gender: Men in HYROX - Saturday\n",
      "[2025 Verona] Page 2 scraped for HYROX - Friday - Men\n",
      "[2025 Stuttgart] Page 1 scraped for HYROX PRO - Friday - Women\n",
      "[2025 Atlanta] Page 4 scraped for HYROX - Saturday - Men\n",
      "[2025 Johannesburg] Page 3 scraped for HYROX - Men\n",
      "[2026 Guadalajara] Page 4 scraped for HYROX - Men\n",
      "[2025 Melbourne] Page 5 scraped for HYROX PRO - Saturday - Men\n",
      "[2025 Stuttgart] Page 2 scraped for HYROX PRO - Friday - Women\n",
      "[2025 Stuttgart] Saved 497 rows for HYROX PRO - Friday\n",
      "[2025 Stuttgart] HYROX - Saturday already exists, will check for data\n",
      "[2025 Stuttgart] Scraping division: HYROX - Saturday\n",
      "[2026 St. Gallen] Page 5 scraped for HYROX - Men\n",
      "[2025 Stuttgart] Scraping gender: Men in HYROX - Saturday\n",
      "[2025 Frankfurt] Page 1 scraped for HYROX - Men\n",
      "[2026 Bilbao] Page 3 scraped for HYROX - Men\n",
      "[2025 Verona] Page 3 scraped for HYROX - Friday - Men\n",
      "[2025 Atlanta] Page 5 scraped for HYROX - Saturday - Men\n",
      "[2025 Verona] Page 4 scraped for HYROX - Friday - Men\n",
      "[2025 Verona] Saved 319 rows for HYROX - Friday\n",
      "[2025 Verona] HYROX - Saturday already exists, will check for data\n",
      "[2025 Verona] Scraping division: HYROX - Saturday\n",
      "[2025 Verona] Scraping gender: Men in HYROX - Saturday\n",
      "[2025 Gent] Page 1 scraped for HYROX - Saturday - Men\n",
      "[2026 Guadalajara] Page 5 scraped for HYROX - Men\n",
      "[2025 Melbourne] Page 6 scraped for HYROX PRO - Saturday - Men\n",
      "[2026 St. Gallen] Page 6 scraped for HYROX - Men\n",
      "[2026 Bilbao] Page 4 scraped for HYROX - Men\n",
      "[2025 Johannesburg] Page 4 scraped for HYROX - Men\n",
      "[2025 Stuttgart] Page 1 scraped for HYROX - Saturday - Men\n",
      "[2025 Atlanta] Page 6 scraped for HYROX - Saturday - Men\n",
      "[2025 Frankfurt] Page 2 scraped for HYROX - Men\n",
      "[2025 Verona] Page 1 scraped for HYROX - Saturday - Men\n",
      "[2026 Guadalajara] Page 6 scraped for HYROX - Men\n",
      "[2025 Melbourne] Page 7 scraped for HYROX PRO - Saturday - Men\n",
      "[2026 St. Gallen] Page 7 scraped for HYROX - Men\n",
      "[2026 Bilbao] Page 5 scraped for HYROX - Men\n",
      "[2025 Stuttgart] Page 2 scraped for HYROX - Saturday - Men\n",
      "[2025 Gent] Page 2 scraped for HYROX - Saturday - Men\n",
      "[2025 Atlanta] Page 7 scraped for HYROX - Saturday - Men\n",
      "[2025 Melbourne] Page 8 scraped for HYROX PRO - Saturday - Men\n",
      "[2025 Melbourne] Scraping gender: Women in HYROX PRO - Saturday\n",
      "[2025 Johannesburg] Page 5 scraped for HYROX - Men\n",
      "[2025 Verona] Page 2 scraped for HYROX - Saturday - Men\n",
      "[2025 Frankfurt] Page 3 scraped for HYROX - Men\n",
      "[2026 St. Gallen] Page 8 scraped for HYROX - Men\n",
      "[2026 Guadalajara] Page 7 scraped for HYROX - Men\n",
      "[2026 Bilbao] Page 6 scraped for HYROX - Men\n",
      "[2025 Stuttgart] Page 3 scraped for HYROX - Saturday - Men\n",
      "[2025 Atlanta] Page 8 scraped for HYROX - Saturday - Men\n",
      "[2025 Atlanta] Saved 788 rows for HYROX - Saturday\n",
      "[2025 Atlanta] Scraping division: HYROX - Sunday\n",
      "[2025 Atlanta] Scraping gender: Men in HYROX - Sunday\n",
      "[2025 Atlanta] Page 1 scraped for HYROX - Sunday - Men\n",
      "[2025 Atlanta] Scraping gender: Women in HYROX - Sunday\n",
      "[2025 Melbourne] Page 1 scraped for HYROX PRO - Saturday - Women\n",
      "[2025 Gent] Page 3 scraped for HYROX - Saturday - Men\n",
      "[2025 Verona] Page 3 scraped for HYROX - Saturday - Men\n",
      "[2026 Guadalajara] Page 8 scraped for HYROX - Men\n",
      "[2026 St. Gallen] Page 9 scraped for HYROX - Men\n",
      "[2026 Bilbao] Page 7 scraped for HYROX - Men\n",
      "[2025 Johannesburg] Page 6 scraped for HYROX - Men\n",
      "[2025 Stuttgart] Page 4 scraped for HYROX - Saturday - Men\n",
      "[2025 Frankfurt] Page 4 scraped for HYROX - Men\n",
      "[2025 Melbourne] Page 2 scraped for HYROX PRO - Saturday - Women\n",
      "[2025 Atlanta] Page 1 scraped for HYROX - Sunday - Women\n",
      "[2025 Johannesburg] Page 7 scraped for HYROX - Men\n",
      "[2025 Johannesburg] Scraping gender: Women in HYROX\n",
      "[2025 Verona] Page 4 scraped for HYROX - Saturday - Men\n",
      "[2026 St. Gallen] Page 10 scraped for HYROX - Men\n",
      "[2026 Guadalajara] Page 9 scraped for HYROX - Men\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "#BASE_URLS = {f\"Season_{i}\": f\"https://results.hyrox.com/season-{i}/\" for i in range(1, 9)}\n",
    "BASE_URLS = {f\"Season_{i}\": f\"https://results.hyrox.com/season-{i}/\" for i in range(8, 9)}\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "MAX_THREADS = 50  # adjust for your system\n",
    "\n",
    "def human_pause(a=2, b=5):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# SELENIUM DRIVER\n",
    "# ==========================\n",
    "\n",
    "def create_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE PAGES FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(driver, race_name, division, gender_label, race_results):\n",
    "    is_doubles = \"DOUBLES\" in division.upper()\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            WebDriverWait(driver, 25).until(lambda d: \n",
    "                \"There are currently no results available\" in d.page_source\n",
    "                or len(d.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")) > 1\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"[{race_name}] Timeout waiting for {division} - {gender_label}\")\n",
    "            return False\n",
    "\n",
    "        # No results?\n",
    "        try:\n",
    "            no_result_elem = driver.find_element(By.XPATH,\n",
    "                \"//*[contains(text(),'There are currently no results available')]\"\n",
    "            )\n",
    "            if no_result_elem.is_displayed():\n",
    "                print(f\"[{race_name}] No results for {division} - {gender_label}\")\n",
    "                return False\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "        rows = [r for r in rows if \"list-group-header\" not in r.get_attribute(\"class\")]\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"[{race_name}] No rows found for {division} - {gender_label}\")\n",
    "            return False\n",
    "\n",
    "        scraped_any = False\n",
    "        for row in rows:\n",
    "            try:\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\", \"\").strip()\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\", \"\").strip()\n",
    "\n",
    "                if is_doubles:\n",
    "                    members = row.find_elements(By.CSS_SELECTOR, \".type-relay_member a\")\n",
    "                    member_names = \" & \".join([m.text for m in members])\n",
    "                    race_results.append([\n",
    "                        race_name, division, gender_label,\n",
    "                        rank, age_rank, member_names, \"\",\n",
    "                        age_group, total_time\n",
    "                    ])\n",
    "                else:\n",
    "                    name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "                    race_results.append([\n",
    "                        race_name, division, gender_label,\n",
    "                        rank, age_rank, name, nation,\n",
    "                        age_group, total_time\n",
    "                    ])\n",
    "                scraped_any = True\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"[{race_name}] Page {page_number} scraped for {division} - {gender_label}\")\n",
    "        page_number += 1\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            human_pause(2,5)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    return scraped_any\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE RACE FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_race(season, base_url, race_name, race_value):\n",
    "    driver = create_driver()\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "    print(f\"\\n[{race_name}] Starting scraping\")\n",
    "\n",
    "    driver.get(base_url)\n",
    "    human_pause(2,5)\n",
    "    Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "    human_pause(1,3)\n",
    "\n",
    "    # Get all divisions\n",
    "    select_division = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "    divisions = [o.text for o in select_division.options]\n",
    "\n",
    "    # Check if all division CSVs exist; if yes, skip the race entirely\n",
    "    all_exist = True\n",
    "    for div in divisions:\n",
    "        div_safe = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        file_path = os.path.join(season_folder, f\"{safe_name}_{div_safe}.csv\")\n",
    "        if not os.path.exists(file_path):\n",
    "            all_exist = False\n",
    "            break\n",
    "    if all_exist:\n",
    "        print(f\"[{race_name}] All division CSVs exist. Skipping race.\")\n",
    "        driver.quit()\n",
    "        return f\"[{race_name}] Skipped (all divisions exist)\"\n",
    "\n",
    "    for div in divisions:\n",
    "        div_safe = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        file_path = os.path.join(season_folder, f\"{safe_name}_{div_safe}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"[{race_name}] {div} already exists, will check for data\")\n",
    "        \n",
    "        print(f\"[{race_name}] Scraping division: {div}\")\n",
    "\n",
    "        driver.get(base_url)\n",
    "        human_pause(2,4)\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "        human_pause(1,2)\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "        human_pause(1,2)\n",
    "\n",
    "        try:\n",
    "            gender_dropdown = Select(driver.find_element(By.ID, \"default-lists-sex\"))\n",
    "            genders = [(o.get_attribute(\"value\"), o.text) for o in gender_dropdown.options]\n",
    "        except NoSuchElementException:\n",
    "            genders = [(\"\", \"All\")]\n",
    "\n",
    "        division_results = []\n",
    "        division_has_data = False\n",
    "\n",
    "        for gender_code, gender_label in genders:\n",
    "            print(f\"[{race_name}] Scraping gender: {gender_label} in {div}\")\n",
    "\n",
    "            driver.get(base_url)\n",
    "            human_pause(2,4)\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "            human_pause(1,2)\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "            human_pause(1,2)\n",
    "            if gender_code:\n",
    "                Select(driver.find_element(By.ID, \"default-lists-sex\")).select_by_value(gender_code)\n",
    "            Select(driver.find_element(By.ID, \"default-num_results\")).select_by_value(\"100\")\n",
    "            human_pause(1,2)\n",
    "            driver.find_element(By.ID, \"default-submit\").click()\n",
    "            human_pause(2,4)\n",
    "\n",
    "            has_data = scrape_pages(driver, race_name, div, gender_label, division_results)\n",
    "            if has_data:\n",
    "                division_has_data = True\n",
    "\n",
    "        # Save CSV even if empty (no data)\n",
    "        df = pd.DataFrame(\n",
    "            division_results,\n",
    "            columns=[\"Race\",\"Division\",\"Gender\",\n",
    "                     \"Rank Overall\",\"Rank Age Group\",\n",
    "                     \"Name\",\"Nation\",\"Age Group\",\"Total Time\"]\n",
    "        )\n",
    "        df.to_csv(file_path, index=False)\n",
    "        if division_has_data:\n",
    "            print(f\"[{race_name}] Saved {len(df)} rows for {div}\")\n",
    "        else:\n",
    "            print(f\"[{race_name}] Division {div} had no data. Empty CSV saved.\")\n",
    "\n",
    "    driver.quit()\n",
    "    return f\"[{race_name}] Finished scraping\"\n",
    "\n",
    "# ==========================\n",
    "# MAIN EXECUTION\n",
    "# ==========================\n",
    "\n",
    "all_tasks = []\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "    driver_main = create_driver()\n",
    "    driver_main.get(base_url)\n",
    "    human_pause(2,4)\n",
    "    try:\n",
    "        WebDriverWait(driver_main, 20).until(EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\")))\n",
    "    except TimeoutException:\n",
    "        driver_main.quit()\n",
    "        continue\n",
    "\n",
    "    race_dropdown = Select(driver_main.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "    races = [(race_dropdown.options[i].text, race_dropdown.options[i].get_attribute(\"value\")) for i in range(len(race_dropdown.options))]\n",
    "    driver_main.quit()\n",
    "\n",
    "    for race_name, race_value in races:\n",
    "        all_tasks.append((season, base_url, race_name, race_value))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    futures = [executor.submit(scrape_race, *task) for task in all_tasks]\n",
    "    for f in futures:\n",
    "        print(f.result())\n",
    "\n",
    "print(\"\\nALL DONE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fbea2-3fc1-4a36-97d7-c16a9f12e03c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c7a920-a66f-4aae-bc60-cb0eb7ded482",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
