{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4703437a-4581-443d-b88a-8fc5d12559b9",
   "metadata": {},
   "source": [
    "## Scraping & Verifying HYROX Results Across Seasons\n",
    "\n",
    "**Competitions:** HYROX Seasons 1‚Äì8 (Official Results Portal)  \n",
    "**Purpose:** Scrape all available race results and verify that every race, division, and gender has been successfully collected  \n",
    "**Methods:** Selenium automation, controlled multi-threading, dynamic pagination handling, structured CSV export, division detection, empty-file tracking, and race-level completeness checks  \n",
    "**Author:** [Victoria Friss de Kereki](https://www.linkedin.com/in/victoria-friss-de-kereki/)  \n",
    "\n",
    "---\n",
    "\n",
    "**Notebook first written:** `23/02/2026`  \n",
    "**Last updated:** `27/02/2026`  \n",
    "\n",
    "> This notebook builds a **robust scraping and verification pipeline** for HYROX competition results.\n",
    "> \n",
    "> The workflow:\n",
    "> \n",
    "> - üåê Scrapes race results directly from the official HYROX results platform  \n",
    "> - üóÇ Organises outputs by **Season, Race, and Division**  \n",
    "> - üîÑ Handles dynamic page loading and pagination safely  \n",
    "> - üìÅ Saves structured CSV files for each race-division combination  \n",
    "> - ‚ö†Ô∏è Tracks empty divisions and failed scrapes  \n",
    "> - üèÅ Verifies that every race includes all the available divisions\n",
    "> \n",
    "> The objective of this notebook is to ensure **complete and reliable data extraction**, creating a solid foundation for downstream cleaning, validation, and analytical modelling in subsequent notebooks.\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ae46c-788b-442b-aec2-9f717e2392a2",
   "metadata": {},
   "source": [
    "### Scrape many at a time, for seasons and divisions available, even the empty ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1418c458-8cfa-41c9-9ada-8af3f467bf23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {f\"Season_{i}\": f\"https://results.hyrox.com/season-{i}/\" for i in range(1, 9)}\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "MAX_THREADS = 20  # adjust as needed\n",
    "\n",
    "def human_pause(a=2, b=5):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# SELENIUM DRIVER\n",
    "# ==========================\n",
    "\n",
    "def create_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE PAGE ROWS\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(driver, race_name, division, gender_label, race_results):\n",
    "    page_number = 1\n",
    "    scraped_any = False\n",
    "\n",
    "    while True:\n",
    "        # Wait until rows appear or \"no results\" text shows\n",
    "        try:\n",
    "            WebDriverWait(driver, 25).until(\n",
    "                lambda d: \"There are currently no results available\" in d.page_source\n",
    "                or len(d.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")) >= 1\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"[{race_name}] Timeout waiting for {division} - {gender_label}\")\n",
    "            return False\n",
    "\n",
    "        # No results message?\n",
    "        try:\n",
    "            no_result_elem = driver.find_element(By.XPATH,\n",
    "                \"//*[contains(text(),'There are currently no results available')]\")\n",
    "            if no_result_elem.is_displayed():\n",
    "                print(f\"[{race_name}] No results for {division} - {gender_label}\")\n",
    "                return False\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "\n",
    "        # Rows\n",
    "        rows = driver.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "        rows = [r for r in rows if \"list-group-header\" not in r.get_attribute(\"class\")]\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"[{race_name}] No rows found for {division} - {gender_label}\")\n",
    "            return False\n",
    "\n",
    "        for row in rows:\n",
    "            try:\n",
    "                # Modern layout\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                try:\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "                except:\n",
    "                    nation = \"\"\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\",\"\").strip()\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\",\"\").strip()\n",
    "            except:\n",
    "                # 2018 fallback layout (Youngstars)\n",
    "                parts = [p.strip() for p in row.text.split(\"\\n\") if p.strip()]\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                rank = parts[0]\n",
    "                age_rank = \"\"\n",
    "                name = parts[2]\n",
    "                age_group = parts[3].replace(\"‚Äì\",\"\").strip()\n",
    "                total_time = parts[4][-8:]\n",
    "                nation = \"\"\n",
    "\n",
    "            race_results.append([\n",
    "                race_name, division, gender_label,\n",
    "                rank, age_rank, name, nation,\n",
    "                age_group, total_time\n",
    "            ])\n",
    "            scraped_any = True\n",
    "\n",
    "        print(f\"[{race_name}] Page {page_number} scraped for {division} - {gender_label}\")\n",
    "        page_number += 1\n",
    "\n",
    "        # Pagination\n",
    "        try:\n",
    "            next_btn = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_btn)\n",
    "            human_pause(2,5)\n",
    "        except NoSuchElementException:\n",
    "            break\n",
    "\n",
    "    return scraped_any\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE A RACE\n",
    "# ==========================\n",
    "def scrape_race(season, base_url, race_name, race_value):\n",
    "    driver = create_driver()\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "    print(f\"\\n[{race_name}] Starting scraping\")\n",
    "\n",
    "    # Get all divisions\n",
    "    driver.get(base_url)\n",
    "    human_pause(2,5)\n",
    "    Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "    human_pause(1,3)\n",
    "    try:\n",
    "        select_division = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "        divisions = [(o.text, o.get_attribute(\"value\")) for o in select_division.options]\n",
    "    except NoSuchElementException:\n",
    "        # Sometimes dropdown is missing (rare), fallback to All\n",
    "        divisions = [(\"All\", \"\")]\n",
    "\n",
    "    # Skip if all CSVs exist\n",
    "    all_exist = all(\n",
    "        os.path.exists(os.path.join(season_folder, f\"{safe_name}_{div[0].replace(' ','_').replace('/','-')}.csv\"))\n",
    "        for div in divisions\n",
    "    )\n",
    "    if all_exist:\n",
    "        print(f\"[{race_name}] All division CSVs exist. Skipping race.\")\n",
    "        driver.quit()\n",
    "        return f\"[{race_name}] Skipped (all divisions exist)\"\n",
    "\n",
    "    for div_name, div_value in divisions:\n",
    "        div_safe = div_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        file_path = os.path.join(season_folder, f\"{safe_name}_{div_safe}.csv\")\n",
    "        if os.path.exists(file_path):\n",
    "            print(f\"[{race_name}] {div_name} already exists. Skipping division.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"[{race_name}] Scraping division: {div_name}\")\n",
    "\n",
    "        # Get genders\n",
    "        try:\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "            human_pause(1,2)\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_value(div_value)\n",
    "            human_pause(1,2)\n",
    "            gender_dropdown = Select(driver.find_element(By.ID, \"default-lists-sex\"))\n",
    "            genders = [\n",
    "                (o.get_attribute(\"value\"), o.text.strip())\n",
    "                for o in gender_dropdown.options\n",
    "                if o.text.strip() and \"All\" not in o.text\n",
    "            ]\n",
    "            if not genders:\n",
    "                genders = [(\"\", \"All\")]\n",
    "        except NoSuchElementException:\n",
    "            genders = [(\"\", \"All\")]\n",
    "\n",
    "        division_results = []\n",
    "        division_has_data = False\n",
    "\n",
    "        for gender_code, gender_label in genders:\n",
    "            print(f\"[{race_name}] Scraping gender: {gender_label} in {div_name}\")\n",
    "\n",
    "            # Reload page to reset all dropdowns\n",
    "            driver.get(base_url)\n",
    "            human_pause(2,3)\n",
    "            Select(driver.find_element(By.ID,\"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "            human_pause(1,2)\n",
    "            try:\n",
    "                Select(driver.find_element(By.ID,\"default-lists-event\")).select_by_value(div_value)\n",
    "                human_pause(1,2)\n",
    "            except NoSuchElementException:\n",
    "                print(f\"[{race_name}] Division dropdown not found for {div_name}, continuing\")\n",
    "\n",
    "            # Select gender\n",
    "            if gender_code:\n",
    "                try:\n",
    "                    Select(driver.find_element(By.ID,\"default-lists-sex\")).select_by_value(gender_code)\n",
    "                    human_pause(1,2)\n",
    "                except NoSuchElementException:\n",
    "                    print(f\"[{race_name}] Gender dropdown not found for {div_name}, using default gender\")\n",
    "\n",
    "            # Ensure Workout = Total\n",
    "            try:\n",
    "                Select(driver.find_element(By.ID,\"default-lists-ranking\")).select_by_visible_text(\"Total\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # Ensure 100 results\n",
    "            try:\n",
    "                Select(driver.find_element(By.ID,\"default-num_results\")).select_by_value(\"100\")\n",
    "            except:\n",
    "                pass\n",
    "            human_pause(1,2)\n",
    "\n",
    "            # Click SHOW RESULTS\n",
    "            try:\n",
    "                submit_btn = driver.find_element(By.ID,\"default-submit\")\n",
    "                driver.execute_script(\"arguments[0].click();\", submit_btn)\n",
    "                print(f\"[{race_name}] Clicked SHOW RESULTS for {div_name} - {gender_label}\")\n",
    "            except NoSuchElementException:\n",
    "                print(f\"[{race_name}] Submit button not found for {div_name} - {gender_label}\")\n",
    "                continue\n",
    "\n",
    "            # Wait for the table headers (Workout, Time, Total)\n",
    "            try:\n",
    "                WebDriverWait(driver, 25).until(\n",
    "                    lambda d: len(d.find_elements(By.XPATH,\n",
    "                        \"//li[contains(@class,'list-group-header')]//div[contains(text(),'Workout')]\"\n",
    "                    )) > 0\n",
    "                    or len(d.find_elements(By.XPATH,\n",
    "                        \"//li[contains(@class,'list-group-header')]//div[contains(text(),'Time')]\"\n",
    "                    )) > 0\n",
    "                    or len(d.find_elements(By.XPATH,\n",
    "                        \"//li[contains(@class,'list-group-header')]//div[contains(text(),'Total')]\"\n",
    "                    )) > 0\n",
    "                )\n",
    "            except TimeoutException:\n",
    "                print(f\"[{race_name}] Results did not load for {div_name} - {gender_label}\")\n",
    "                debug_file = os.path.join(season_folder, f\"debug_{safe_name}_{div_safe}_{gender_label}.html\")\n",
    "                with open(debug_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(driver.page_source)\n",
    "                continue\n",
    "\n",
    "            # Scrape the results\n",
    "            has_data = scrape_pages(driver, race_name, div_name, gender_label, division_results)\n",
    "            if has_data:\n",
    "                division_has_data = True\n",
    "\n",
    "        # Save CSV even if empty\n",
    "        df = pd.DataFrame(\n",
    "            division_results,\n",
    "            columns=[\"Race\",\"Division\",\"Gender\",\n",
    "                     \"Rank Overall\",\"Rank Age Group\",\n",
    "                     \"Name\",\"Nation\",\"Age Group\",\"Total Time\"]\n",
    "        )\n",
    "        df.to_csv(file_path, index=False)\n",
    "        if division_has_data:\n",
    "            print(f\"[{race_name}] Saved {len(df)} rows for {div_name}\")\n",
    "        else:\n",
    "            print(f\"[{race_name}] Division {div_name} had no data. Empty CSV saved.\")\n",
    "\n",
    "    driver.quit()\n",
    "    return f\"[{race_name}] Finished scraping\"\n",
    "    \n",
    "# ==========================\n",
    "# MAIN EXECUTION\n",
    "# ==========================\n",
    "\n",
    "all_tasks = []\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "    driver_main = create_driver()\n",
    "    driver_main.get(base_url)\n",
    "    human_pause(2,4)\n",
    "    try:\n",
    "        WebDriverWait(driver_main, 20).until(\n",
    "            EC.presence_of_element_located((By.ID,\"default-lists-event_main_group\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        driver_main.quit()\n",
    "        continue\n",
    "\n",
    "    race_dropdown = Select(driver_main.find_element(By.ID,\"default-lists-event_main_group\"))\n",
    "    races = [(race_dropdown.options[i].text,race_dropdown.options[i].get_attribute(\"value\"))\n",
    "             for i in range(len(race_dropdown.options))]\n",
    "    driver_main.quit()\n",
    "\n",
    "    for race_name, race_value in races:\n",
    "        all_tasks.append((season, base_url, race_name, race_value))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "    futures = [executor.submit(scrape_race,*task) for task in all_tasks]\n",
    "    for f in futures:\n",
    "        print(f.result())\n",
    "\n",
    "print(\"\\nALL DONE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7a28b-16b1-4213-9b5f-691136e63913",
   "metadata": {},
   "source": [
    "### Check all existing seasons/races/divisions have been downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fbea2-3fc1-4a36-97d7-c16a9f12e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {f\"Season_{i}\": f\"https://results.hyrox.com/season-{i}/\" for i in range(1, 9)}\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "\n",
    "# ==========================\n",
    "# DRIVER\n",
    "# ==========================\n",
    "\n",
    "def create_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_argument(\"--start-maximized\")\n",
    "    options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    return webdriver.Chrome(options=options)\n",
    "\n",
    "# ==========================\n",
    "# VERIFICATION\n",
    "# ==========================\n",
    "\n",
    "def verify_all_downloads():\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"VERIFYING HYROX DATASET\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "    for season, base_url in BASE_URLS.items():\n",
    "\n",
    "        print(f\"\\n========== CHECKING {season} ==========\")\n",
    "\n",
    "        season_folder = os.path.join(SAVE_ROOT, season)\n",
    "\n",
    "        if not os.path.exists(season_folder):\n",
    "            print(f\"‚ùå Season folder missing: {season}\")\n",
    "            continue\n",
    "\n",
    "        local_files = set(os.listdir(season_folder))\n",
    "\n",
    "        driver = create_driver()\n",
    "        driver.get(base_url)\n",
    "\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(\n",
    "                \n",
    "                EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\"))\n",
    "            )\n",
    "        except TimeoutException:\n",
    "            print(f\"‚ùå Could not load season page: {season}\")\n",
    "            driver.quit()\n",
    "            continue\n",
    "\n",
    "        race_dropdown = Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "        races = [(opt.text.strip(), opt.get_attribute(\"value\"))\n",
    "                 for opt in race_dropdown.options]\n",
    "\n",
    "        missing_races = []\n",
    "        missing_divisions = []\n",
    "\n",
    "        for race_name, race_value in races:\n",
    "\n",
    "            safe_race = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "            print(f\"Checking race: {race_name}\")\n",
    "\n",
    "            driver.get(base_url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\\\n",
    "                .select_by_value(race_value)\n",
    "            time.sleep(2)\n",
    "\n",
    "            division_dropdown = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "            divisions = [opt.text.strip() for opt in division_dropdown.options]\n",
    "\n",
    "            race_files = [f for f in local_files if f.startswith(safe_race + \"_\")]\n",
    "\n",
    "            if not race_files:\n",
    "                missing_races.append(race_name)\n",
    "\n",
    "            race_missing_divs = []\n",
    "\n",
    "            for div in divisions:\n",
    "                safe_div = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "                expected_filename = f\"{safe_race}_{safe_div}.csv\"\n",
    "\n",
    "                if expected_filename not in local_files:\n",
    "                    race_missing_divs.append(div)\n",
    "\n",
    "            if race_missing_divs:\n",
    "                missing_divisions.append((race_name, race_missing_divs))\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        # REPORT\n",
    "        if not missing_races:\n",
    "            print(\"‚úÖ No missing races.\")\n",
    "        else:\n",
    "            print(\"\\n‚ùå Missing races:\")\n",
    "            for r in missing_races:\n",
    "                print(f\"   - {r}\")\n",
    "\n",
    "        if not missing_divisions:\n",
    "            print(\"‚úÖ All divisions present.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö† Missing divisions:\")\n",
    "            for race, divs in missing_divisions:\n",
    "                print(f\"\\n  {race}\")\n",
    "                for d in divs:\n",
    "                    print(f\"     - {d}\")\n",
    "\n",
    "        print(f\"\\n========== DONE {season} ==========\")\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(\"VERIFICATION COMPLETE\")\n",
    "    print(\"==============================\")\n",
    "\n",
    "# ==========================\n",
    "# RUN\n",
    "# ==========================\n",
    "\n",
    "verify_all_downloads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0c48a6-c928-4801-8811-cd277858ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# STEP 3: Validate Empty Files Coverage\n",
    "# ---------------------------------\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"EMPTY FILE VALIDATION\")\n",
    "print(\"============================\")\n",
    "\n",
    "# Map base race+division ‚Üí files with data\n",
    "coverage_map = defaultdict(list)\n",
    "\n",
    "for division, files in division_files.items():\n",
    "\n",
    "    for file_path, filename_without_ext, season in files:\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # Remove day/overall suffix for base grouping\n",
    "            base_name = re.sub(r\"_(OVERALL|SATURDAY|SUNDAY|FRIDAY|THURSDAY|WEDNESDAY)$\",\n",
    "                               \"\",\n",
    "                               filename_without_ext.upper())\n",
    "\n",
    "            coverage_map[base_name].append(filename_without_ext)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "problems_found = False\n",
    "\n",
    "for empty_file in empty_files_detected:\n",
    "\n",
    "    empty_upper = empty_file.upper()\n",
    "\n",
    "    base_name = re.sub(r\"_(OVERALL|SATURDAY|SUNDAY|FRIDAY|THURSDAY|WEDNESDAY)$\",\n",
    "                       \"\",\n",
    "                       empty_upper)\n",
    "\n",
    "    if base_name in coverage_map and len(coverage_map[base_name]) > 0:\n",
    "        print(f\"‚úÖ OK: {empty_file} is empty but covered by:\")\n",
    "        for alt in coverage_map[base_name]:\n",
    "            print(f\"   ‚Ü≥ {alt}\")\n",
    "    else:\n",
    "        print(f\"‚ùå MISSING DATA: {empty_file} has no alternative dataset with data\")\n",
    "        problems_found = True\n",
    "\n",
    "if not empty_files_detected:\n",
    "    print(\"No empty files to validate.\")\n",
    "\n",
    "elif not problems_found:\n",
    "    print(\"\\nAll empty files are safely covered by alternative datasets.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö† Some races/divisions are completely missing data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef42223-73f3-4adb-ae3f-16389ee0c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_folder = r\"Datasets\\Hyrox\"\n",
    "\n",
    "deleted_files = []\n",
    "failed_files = []\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"DELETING EMPTY CSV FILES\")\n",
    "print(\"============================\")\n",
    "\n",
    "for subdir, _, files in os.walk(root_folder):\n",
    "\n",
    "    # Skip processed dataset folder\n",
    "    if \"processed dataset\" in subdir.lower():\n",
    "        continue\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        if not file.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(subdir, file)\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty:\n",
    "                os.remove(file_path)\n",
    "                deleted_files.append(file_path)\n",
    "                print(f\"üóë Deleted: {file_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            failed_files.append((file_path, str(e)))\n",
    "\n",
    "# ---------------------------------\n",
    "# SUMMARY\n",
    "# ---------------------------------\n",
    "print(\"\\n============================\")\n",
    "print(\"DELETION SUMMARY\")\n",
    "print(\"============================\")\n",
    "\n",
    "print(f\"Total empty files deleted: {len(deleted_files)}\")\n",
    "\n",
    "if failed_files:\n",
    "    print(\"\\nFiles that could not be processed:\")\n",
    "    for path, error in failed_files:\n",
    "        print(f\"- {path} ({error})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853f3f02-b922-49c4-8e37-3314ae1fe9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "root_folder = r\"Datasets\\Hyrox\"\n",
    "output_folder = os.path.join(root_folder, \"Processed dataset\")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "division_files = defaultdict(list)\n",
    "division_datasets = {}\n",
    "\n",
    "suffix_words = {\n",
    "    \"overall\", \"saturday\", \"sunday\", \"friday\",\n",
    "    \"thursday\", \"wednesday\"\n",
    "}\n",
    "\n",
    "# ---------------------------------\n",
    "# STEP 1: Group files by division\n",
    "# ---------------------------------\n",
    "for subdir, _, files in os.walk(root_folder):\n",
    "\n",
    "    # Skip processed dataset folder\n",
    "    if \"processed dataset\" in subdir.lower():\n",
    "        continue\n",
    "\n",
    "    season = os.path.basename(subdir)\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        if not file.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        filename_without_ext = file.replace(\".csv\", \"\")\n",
    "\n",
    "        # Skip generic names if they exist\n",
    "        if filename_without_ext.lower() in [\"doubles\", \"singles\", \"hyrox_pro\"]:\n",
    "            continue\n",
    "\n",
    "        parts = filename_without_ext.split(\"_\")\n",
    "\n",
    "        # Find all occurrences of \"hyrox\"\n",
    "        hyrox_indices = [i for i, p in enumerate(parts) if p.lower() == \"hyrox\"]\n",
    "\n",
    "        if not hyrox_indices:\n",
    "            continue\n",
    "\n",
    "        # Take LAST occurrence (fixes London issue)\n",
    "        hyrox_index = hyrox_indices[-1]\n",
    "\n",
    "        division_parts = parts[hyrox_index:]\n",
    "\n",
    "        # Remove unwanted suffixes\n",
    "        while division_parts and division_parts[-1].lower() in suffix_words:\n",
    "            division_parts = division_parts[:-1]\n",
    "\n",
    "        division = \"_\".join(division_parts)\n",
    "\n",
    "        # Standardise formatting\n",
    "        division = division.upper()\n",
    "        division = division.replace(\"-\", \"_\")\n",
    "        division = re.sub(r\"_+$\", \"\", division)\n",
    "        division = re.sub(r\"_+\", \"_\", division)\n",
    "\n",
    "        file_path = os.path.join(subdir, file)\n",
    "\n",
    "        division_files[division].append((file_path, filename_without_ext, season))\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# STEP 2: Create dataset per division\n",
    "# ---------------------------------\n",
    "for division, files in division_files.items():\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for file_path, filename_without_ext, season in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty:\n",
    "                continue   # Just skip empty files silently\n",
    "\n",
    "            df[\"source_file\"] = filename_without_ext\n",
    "            df[\"Season\"] = season\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{division}.csv\")\n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "        division_datasets[division] = combined_df\n",
    "        globals()[division] = combined_df\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# FINAL SUMMARY\n",
    "# ---------------------------------\n",
    "print(\"\\n============================\")\n",
    "print(\"DATASETS CREATED\")\n",
    "print(\"============================\")\n",
    "\n",
    "if division_datasets:\n",
    "    for name in sorted(division_datasets.keys()):\n",
    "        print(f\"{name} ‚Üí {len(division_datasets[name])} rows\")\n",
    "else:\n",
    "    print(\"No datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b28bd-e6c7-4d7c-a805-eacb4b48b286",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
