{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "469732e2-a9f6-4df0-b75b-bc6d63f4cedb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STARTING Season_7\n",
      "==============================\n",
      "\n",
      "Found 73 races in Season_7\n",
      "\n",
      "Processing race: 2025 Shanghai\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Shanghai_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Shanghai_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Atlanta\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Atlanta_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Atlanta_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Valencia\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Valencia_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Valencia_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Maastricht\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Maastricht_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Maastricht_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Mumbai\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Mumbai_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Mumbai_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 World Championships\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_World_Championships_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_World_Championships_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Cardiff\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Cardiff_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Cardiff_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 New York\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_New_York_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_New_York_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Riga\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Riga_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Riga_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Rimini\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Rimini_HYROX_PRO_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Bangkok\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Berlin\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Berlin_HYROX_PRO_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Incheon\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Incheon_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Incheon_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Heerenveen\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 London\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_London_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_London_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Barcelona\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Miami Beach\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Miami_Beach_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Miami_Beach_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Paris 1\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Paris_1_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Paris_1_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Cologne\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Cologne_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Cologne_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Sharjah\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Sharjah_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Sharjah_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Taipei\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Taipei_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Taipei_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Warsaw\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Warsaw_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Warsaw_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Belgium\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Belgium_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Belgium_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Monterrey\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Monterrey_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Monterrey_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Washington D.C. Open North American Championships\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Malaga\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Malaga_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Malaga_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Glasgow\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Glasgow_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Glasgow_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Copenhagen\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Copenhagen_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Copenhagen_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Houston\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Houston_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Houston_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Brisbane\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Brisbane_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Brisbane_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Johannesburg I\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Johannesburg_I_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Johannesburg_I_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Karlsruhe\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Karlsruhe_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Karlsruhe_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Rotterdam\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Rotterdam_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Rotterdam_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Vienna\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Katowice\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Katowice_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Katowice_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Bilbao\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Bilbao_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Bilbao_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Guadalajara\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Guadalajara_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Guadalajara_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 St. Gallen\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_St._Gallen_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_St._Gallen_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Toulouse\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Toulouse_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Toulouse_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Auckland\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Auckland_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Auckland_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Las Vegas\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Las_Vegas_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Las_Vegas_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Turin\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Turin_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Turin_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2025 Manchester\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Manchester_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2025_Manchester_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Frankfurt\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Frankfurt_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Frankfurt_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Melbourne\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Melbourne_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Melbourne_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Anaheim\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Anaheim_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Anaheim_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Marseille\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Marseille_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Marseille_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Stockholm\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Stockholm_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Stockholm_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2024 London\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_London_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_London_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Hong Kong\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2024 Dallas\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Dallas_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Dallas_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Beijing\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Beijing_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Beijing_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Chicago Navy Pier\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Chicago_Navy_Pier_HYROX_PRO.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Chicago_Navy_Pier_HYROX_PRO_DOUBLES.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Dublin\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Dublin_HYROX_PRO_-_Overall.csv, skipping division\n",
      "   Division CSV already exists: Datasets\\Hyrox\\Season_7\\2024_Dublin_HYROX_PRO_DOUBLES_-_Overall.csv, skipping division\n",
      "\n",
      "Processing race: 2024 Manchester\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 230 rows to Datasets\\Hyrox\\Season_7\\2024_Manchester_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 138 rows to Datasets\\Hyrox\\Season_7\\2024_Manchester_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Ciudad de Mexico\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 353 rows to Datasets\\Hyrox\\Season_7\\2024_Ciudad_de_Mexico_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 98 rows to Datasets\\Hyrox\\Season_7\\2024_Ciudad_de_Mexico_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Paris\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 259 rows to Datasets\\Hyrox\\Season_7\\2024_Paris_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 195 rows to Datasets\\Hyrox\\Season_7\\2024_Paris_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Poznan\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 262 rows to Datasets\\Hyrox\\Season_7\\2024_Poznan_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 77 rows to Datasets\\Hyrox\\Season_7\\2024_Poznan_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Hamburg\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 262 rows to Datasets\\Hyrox\\Season_7\\2024_Hamburg_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 174 rows to Datasets\\Hyrox\\Season_7\\2024_Hamburg_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Birmingham\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 522 rows to Datasets\\Hyrox\\Season_7\\2024_Birmingham_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 291 rows to Datasets\\Hyrox\\Season_7\\2024_Birmingham_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2024 Madrid\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 254 rows to Datasets\\Hyrox\\Season_7\\2024_Madrid_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 144 rows to Datasets\\Hyrox\\Season_7\\2024_Madrid_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Incheon\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 122 rows to Datasets\\Hyrox\\Season_7\\2024_Incheon_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 114 rows to Datasets\\Hyrox\\Season_7\\2024_Incheon_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Milan\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 393 rows to Datasets\\Hyrox\\Season_7\\2024_Milan_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 181 rows to Datasets\\Hyrox\\Season_7\\2024_Milan_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Amsterdam\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 451 rows to Datasets\\Hyrox\\Season_7\\2024_Amsterdam_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 272 rows to Datasets\\Hyrox\\Season_7\\2024_Amsterdam_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2024 Nice\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 212 rows to Datasets\\Hyrox\\Season_7\\2024_Nice_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 79 rows to Datasets\\Hyrox\\Season_7\\2024_Nice_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Toronto\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 323 rows to Datasets\\Hyrox\\Season_7\\2024_Toronto_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 107 rows to Datasets\\Hyrox\\Season_7\\2024_Toronto_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2024 Stuttgart\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 214 rows to Datasets\\Hyrox\\Season_7\\2024_Stuttgart_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 103 rows to Datasets\\Hyrox\\Season_7\\2024_Stuttgart_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Perth\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 152 rows to Datasets\\Hyrox\\Season_7\\2024_Perth_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 47 rows to Datasets\\Hyrox\\Season_7\\2024_Perth_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Cape Town\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 118 rows to Datasets\\Hyrox\\Season_7\\2024_Cape_Town_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 21 rows to Datasets\\Hyrox\\Season_7\\2024_Cape_Town_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Singapore\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 185 rows to Datasets\\Hyrox\\Season_7\\2024_Singapore_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 189 rows to Datasets\\Hyrox\\Season_7\\2024_Singapore_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Brisbane\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 212 rows to Datasets\\Hyrox\\Season_7\\2024_Brisbane_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 101 rows to Datasets\\Hyrox\\Season_7\\2024_Brisbane_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Sydney\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 179 rows to Datasets\\Hyrox\\Season_7\\2024_Sydney_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 78 rows to Datasets\\Hyrox\\Season_7\\2024_Sydney_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2024 Singapore National Stadium\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 150 rows to Datasets\\Hyrox\\Season_7\\2024_Singapore_National_Stadium_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 94 rows to Datasets\\Hyrox\\Season_7\\2024_Singapore_National_Stadium_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "==============================\n",
      "STARTING Season_8\n",
      "==============================\n",
      "\n",
      "Found 65 races in Season_8\n",
      "\n",
      "Processing race: 2026 Taipei\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Fortaleza\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_8\\2026_Fortaleza_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_8\\2026_Fortaleza_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2026 Washington DC\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Las Vegas\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Katowice\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 416 rows to Datasets\\Hyrox\\Season_8\\2026_Katowice_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 188 rows to Datasets\\Hyrox\\Season_8\\2026_Katowice_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2026 Istanbul\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 248 rows to Datasets\\Hyrox\\Season_8\\2026_Istanbul_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 128 rows to Datasets\\Hyrox\\Season_8\\2026_Istanbul_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2026 Nice\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 538 rows to Datasets\\Hyrox\\Season_8\\2026_Nice_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 468 rows to Datasets\\Hyrox\\Season_8\\2026_Nice_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2026 Bilbao\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 388 rows to Datasets\\Hyrox\\Season_8\\2026_Bilbao_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 232 rows to Datasets\\Hyrox\\Season_8\\2026_Bilbao_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2026 Guadalajara\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 318 rows to Datasets\\Hyrox\\Season_8\\2026_Guadalajara_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 213 rows to Datasets\\Hyrox\\Season_8\\2026_Guadalajara_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2026 Vienna\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 318 rows to Datasets\\Hyrox\\Season_8\\2026_Vienna_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 446 rows to Datasets\\Hyrox\\Season_8\\2026_Vienna_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2026 Phoenix\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Auckland\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Osaka\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Turin\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 549 rows to Datasets\\Hyrox\\Season_8\\2026_Turin_HYROX_PRO_-_Overall.csv\n",
      "\n",
      "Processing race: 2026 Amsterdam\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 682 rows to Datasets\\Hyrox\\Season_8\\2026_Amsterdam_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 734 rows to Datasets\\Hyrox\\Season_8\\2026_Amsterdam_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2026 Amsterdam - Youngstars\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2026 Manchester\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "   Saved 749 rows to Datasets\\Hyrox\\Season_8\\2026_Manchester_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 678 rows to Datasets\\Hyrox\\Season_8\\2026_Manchester_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2026 St. Gallen\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 219 rows to Datasets\\Hyrox\\Season_8\\2026_St._Gallen_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Page 6 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 690 rows to Datasets\\Hyrox\\Season_8\\2026_St._Gallen_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Stockholm\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 550 rows to Datasets\\Hyrox\\Season_8\\2025_Stockholm_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 435 rows to Datasets\\Hyrox\\Season_8\\2025_Stockholm_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2025 Shenzhen\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 368 rows to Datasets\\Hyrox\\Season_8\\2025_Shenzhen_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 216 rows to Datasets\\Hyrox\\Season_8\\2025_Shenzhen_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Vancouver\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 274 rows to Datasets\\Hyrox\\Season_8\\2025_Vancouver_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 109 rows to Datasets\\Hyrox\\Season_8\\2025_Vancouver_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Anaheim\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 511 rows to Datasets\\Hyrox\\Season_8\\2025_Anaheim_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 381 rows to Datasets\\Hyrox\\Season_8\\2025_Anaheim_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Frankfurt\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 428 rows to Datasets\\Hyrox\\Season_8\\2025_Frankfurt_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 619 rows to Datasets\\Hyrox\\Season_8\\2025_Frankfurt_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Melbourne\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Gent\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Poznan\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 478 rows to Datasets\\Hyrox\\Season_8\\2025_Poznan_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_8\\2025_Poznan_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 London Excel\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Page 6 scraped\n",
      "      Page 7 scraped\n",
      "      Page 8 scraped\n",
      "      Page 9 scraped\n",
      "      Page 10 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Page 6 scraped\n",
      "      Last page reached\n",
      "   Saved 1527 rows to Datasets\\Hyrox\\Season_8\\2025_London_Excel_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Page 6 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Page 6 scraped\n",
      "      Last page reached\n",
      "   Saved 1136 rows to Datasets\\Hyrox\\Season_8\\2025_London_Excel_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2025 Verona\n",
      "   No valid HYROX PRO divisions found for this race\n",
      "\n",
      "Processing race: 2025 Johannesburg\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 175 rows to Datasets\\Hyrox\\Season_8\\2025_Johannesburg_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 143 rows to Datasets\\Hyrox\\Season_8\\2025_Johannesburg_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Utrecht\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 498 rows to Datasets\\Hyrox\\Season_8\\2025_Utrecht_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 397 rows to Datasets\\Hyrox\\Season_8\\2025_Utrecht_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2025 Madrid\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Page 5 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 707 rows to Datasets\\Hyrox\\Season_8\\2025_Madrid_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 481 rows to Datasets\\Hyrox\\Season_8\\2025_Madrid_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2025 Rio de Janeiro\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 237 rows to Datasets\\Hyrox\\Season_8\\2025_Rio_de_Janeiro_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_8\\2025_Rio_de_Janeiro_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Singapore Expo\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 331 rows to Datasets\\Hyrox\\Season_8\\2025_Singapore_Expo_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_8\\2025_Singapore_Expo_HYROX_PRO_DOUBLES.csv\n",
      "\n",
      "Processing race: 2025 Bordeaux\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 569 rows to Datasets\\Hyrox\\Season_8\\2025_Bordeaux_HYROX_PRO_-_Overall.csv\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 480 rows to Datasets\\Hyrox\\Season_8\\2025_Bordeaux_HYROX_PRO_DOUBLES_-_Overall.csv\n",
      "\n",
      "Processing race: 2025 Dallas\n",
      "   Division: HYROX PRO\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "   Saved 540 rows to Datasets\\Hyrox\\Season_8\\2025_Dallas_HYROX_PRO.csv\n",
      "   Division: HYROX PRO DOUBLES\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=145.0.7632.76); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff66ae0aa55\n\t0x7ff66ab68630\n\t0x7ff66a8fd75d\n\t0x7ff66a8e8c72\n\t0x7ff66a90f015\n\t0x7ff66a9890e0\n\t0x7ff66a9a5282\n\t0x7ff66a949098\n\t0x7ff66a949f83\n\t0x7ff66ae37810\n\t0x7ff66ae31afd\n\t0x7ff66ae52c1a\n\t0x7ff66ab83345\n\t0x7ff66ab8b81c\n\t0x7ff66ab71924\n\t0x7ff66ab71ad6\n\t0x7ff66ab57e47\n\t0x7ff8b1bae8d7\n\t0x7ff8b33cc40c\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 184\u001b[0m\n\u001b[0;32m    181\u001b[0m     human_pause(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Scrape results\u001b[39;00m\n\u001b[1;32m--> 184\u001b[0m     scrape_pages(race_name, div, gender_name, division_results)\n\u001b[0;32m    185\u001b[0m     human_pause(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;66;03m# Save CSV after both genders are scraped\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[39], line 58\u001b[0m, in \u001b[0;36mscrape_pages\u001b[1;34m(race_name, division, gender, race_results)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         wait\u001b[38;5;241m.\u001b[39muntil(EC\u001b[38;5;241m.\u001b[39mpresence_of_element_located((By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv[data-sex=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgender[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutException:\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m      No results container found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\support\\wait.py:113\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[1;34m(self, method, message)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         value \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_driver)\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[0;32m    115\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\support\\expected_conditions.py:92\u001b[0m, in \u001b[0;36mpresence_of_element_located.<locals>._predicate\u001b[1;34m(driver)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m driver\u001b[38;5;241m.\u001b[39mfind_element(\u001b[38;5;241m*\u001b[39mlocator)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:802\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[1;34m(self, by, value)\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 802\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecute(Command\u001b[38;5;241m.\u001b[39mFIND_ELEMENT, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musing\u001b[39m\u001b[38;5;124m\"\u001b[39m: by, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:432\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    429\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    433\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mInvalidSessionIdException\u001b[0m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=145.0.7632.76); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff66ae0aa55\n\t0x7ff66ab68630\n\t0x7ff66a8fd75d\n\t0x7ff66a8e8c72\n\t0x7ff66a90f015\n\t0x7ff66a9890e0\n\t0x7ff66a9a5282\n\t0x7ff66a949098\n\t0x7ff66a949f83\n\t0x7ff66ae37810\n\t0x7ff66ae31afd\n\t0x7ff66ae52c1a\n\t0x7ff66ab83345\n\t0x7ff66ab8b81c\n\t0x7ff66ab71924\n\t0x7ff66ab71ad6\n\t0x7ff66ab57e47\n\t0x7ff8b1bae8d7\n\t0x7ff8b33cc40c\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {\n",
    "    \"Season_7\": \"https://results.hyrox.com/season-7/?pid=start\",\n",
    "    \"Season_8\": \"https://results.hyrox.com/season-8/\"\n",
    "}\n",
    "\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "\n",
    "DIVISION_OPTIONS = [\n",
    "    \"HYROX PRO\",\n",
    "    \"HYROX PRO - Overall\",\n",
    "    \"HYROX PRO DOUBLES\",\n",
    "    \"HYROX PRO DOUBLES - Overall\"\n",
    "]\n",
    "\n",
    "def human_pause(a=3, b=7):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# DRIVER\n",
    "# ==========================\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE PAGES FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(race_name, division, gender, race_results):\n",
    "    page_number = 1\n",
    "    is_doubles = \"DOUBLES\" in division.upper()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")))\n",
    "        except TimeoutException:\n",
    "            print(\"      No results container found\")\n",
    "            return\n",
    "\n",
    "        human_pause(2, 5)\n",
    "\n",
    "        try:\n",
    "            container = driver.find_element(By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"      No container for this gender\")\n",
    "            return\n",
    "\n",
    "        rows = container.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "        if not rows:\n",
    "            print(\"      No rows found in container\")\n",
    "            return\n",
    "\n",
    "        for row in rows:\n",
    "            if \"list-group-header\" in row.get_attribute(\"class\"):\n",
    "                continue\n",
    "            try:\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\", \"\").strip()\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\", \"\").strip()\n",
    "\n",
    "                if is_doubles:\n",
    "                    # For DOUBLES, get member names\n",
    "                    members = row.find_elements(By.CSS_SELECTOR, \".type-relay_member a\")\n",
    "                    member_names = \" & \".join([m.text for m in members])\n",
    "                    race_results.append([race_name, division, gender, rank, age_rank, member_names, \"\", age_group, total_time])\n",
    "                else:\n",
    "                    # Singles\n",
    "                    name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "                    race_results.append([race_name, division, gender, rank, age_rank, name, nation, age_group, total_time])\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"      Page {page_number} scraped\")\n",
    "        human_pause(3, 6)\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            page_number += 1\n",
    "            human_pause(5, 10)\n",
    "        except NoSuchElementException:\n",
    "            print(\"      Last page reached\")\n",
    "            break\n",
    "\n",
    "# ==========================\n",
    "# MAIN LOOP\n",
    "# ==========================\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "    print(f\"\\n==============================\\nSTARTING {season}\\n==============================\\n\")\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    driver.get(base_url)\n",
    "    human_pause(5, 10)\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\")))\n",
    "    except TimeoutException:\n",
    "        print(f\"No races found for {season}\")\n",
    "        continue\n",
    "\n",
    "    race_dropdown = Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "    races = [(opt.text, opt.get_attribute(\"value\")) for opt in race_dropdown.options]\n",
    "    print(f\"Found {len(races)} races in {season}\")\n",
    "\n",
    "    for race_name, race_value in races:\n",
    "        safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        print(f\"\\nProcessing race: {race_name}\")\n",
    "\n",
    "        # ==========================\n",
    "        # Check available divisions\n",
    "        # ==========================\n",
    "        driver.get(base_url)\n",
    "        human_pause(5, 9)\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "        human_pause(3, 6)\n",
    "        select_division = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "        available_divisions = [o.text for o in select_division.options]\n",
    "        valid_divisions = [d for d in DIVISION_OPTIONS if d in available_divisions]\n",
    "\n",
    "        if not valid_divisions:\n",
    "            print(\"   No valid HYROX PRO divisions found for this race\")\n",
    "            continue\n",
    "\n",
    "        for div in valid_divisions:\n",
    "            div_safe_name = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "            file_path_div = os.path.join(season_folder, f\"{safe_name}_{div_safe_name}.csv\")\n",
    "\n",
    "            if os.path.exists(file_path_div):\n",
    "                print(f\"   Division CSV already exists: {file_path_div}, skipping division\")\n",
    "                continue  # skip this division entirely\n",
    "\n",
    "            print(f\"   Division: {div}\")\n",
    "            division_results = []\n",
    "\n",
    "            for gender_code, gender_name in [(\"M\", \"Men\"), (\"W\", \"Women\")]:\n",
    "                print(f\"      Gender: {gender_name}\")\n",
    "\n",
    "                # Go back to base page and re-select race + division\n",
    "                driver.get(base_url)\n",
    "                human_pause(4, 8)\n",
    "                Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "                human_pause(2, 5)\n",
    "                Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "                human_pause(2, 5)\n",
    "\n",
    "                # Select gender and number of results\n",
    "                Select(driver.find_element(By.ID, \"default-lists-sex\")).select_by_value(gender_code)\n",
    "                Select(driver.find_element(By.ID, \"default-num_results\")).select_by_value(\"100\")\n",
    "                human_pause(2, 5)\n",
    "\n",
    "                # Submit form\n",
    "                driver.find_element(By.ID, \"default-submit\").click()\n",
    "                human_pause(5, 10)\n",
    "\n",
    "                # Scrape results\n",
    "                scrape_pages(race_name, div, gender_name, division_results)\n",
    "                human_pause(2, 5)\n",
    "\n",
    "            # Save CSV after both genders are scraped\n",
    "            columns = [\"Race\", \"Division\", \"Gender\", \"Rank Overall\", \"Rank Age Group\",\n",
    "                       \"Name\", \"Nation\", \"Age Group\", \"Total Time\"]\n",
    "            df_div = pd.DataFrame(division_results, columns=columns)\n",
    "            df_div.to_csv(file_path_div, index=False)\n",
    "            print(f\"   Saved {len(df_div)} rows to {file_path_div}\")\n",
    "            human_pause(3, 6)\n",
    "\n",
    "print(\"\\nALL DONE.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44c7027f-8e8c-47da-ada7-0586f732adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STARTING Season_7\n",
      "==============================\n",
      "\n",
      "Found 73 races in Season_7\n",
      "   Skipping 2025 Shanghai, all divisions already downloaded\n",
      "   Skipping 2025 Atlanta, all divisions already downloaded\n",
      "   Skipping 2025 Valencia, all divisions already downloaded\n",
      "   Skipping 2025 Maastricht, all divisions already downloaded\n",
      "   Skipping 2025 Mumbai, all divisions already downloaded\n",
      "   Skipping 2025 World Championships, all divisions already downloaded\n",
      "   Skipping 2025 Cardiff, all divisions already downloaded\n",
      "   Skipping 2025 New York, all divisions already downloaded\n",
      "   Skipping 2025 Riga, all divisions already downloaded\n",
      "   Skipping 2025 Rimini, all divisions already downloaded\n",
      "   Skipping 2025 Bangkok, all divisions already downloaded\n",
      "   Skipping 2025 Berlin, all divisions already downloaded\n",
      "   Skipping 2025 Incheon, all divisions already downloaded\n",
      "   Skipping 2025 Heerenveen, all divisions already downloaded\n",
      "   Skipping 2025 London, all divisions already downloaded\n",
      "   Skipping 2025 Barcelona, all divisions already downloaded\n",
      "   Skipping 2025 Miami Beach, all divisions already downloaded\n",
      "   Skipping 2025 Paris 1, all divisions already downloaded\n",
      "   Skipping 2025 Cologne, all divisions already downloaded\n",
      "   Skipping 2025 Sharjah, all divisions already downloaded\n",
      "   Skipping 2025 Taipei, all divisions already downloaded\n",
      "   Skipping 2025 Warsaw, all divisions already downloaded\n",
      "   Skipping 2025 Belgium, all divisions already downloaded\n",
      "   Skipping 2025 Monterrey, all divisions already downloaded\n",
      "   Skipping 2025 Washington D.C. Open North American Championships, all divisions already downloaded\n",
      "   Skipping 2025 Malaga, all divisions already downloaded\n",
      "   Skipping 2025 Copenhagen, all divisions already downloaded\n",
      "   Skipping 2025 Houston, all divisions already downloaded\n",
      "   Skipping 2025 Brisbane, all divisions already downloaded\n",
      "   Skipping 2025 Johannesburg I, all divisions already downloaded\n",
      "   Skipping 2025 Karlsruhe, all divisions already downloaded\n",
      "   Skipping 2025 Rotterdam, all divisions already downloaded\n",
      "   Skipping 2025 Vienna, all divisions already downloaded\n",
      "   Skipping 2025 Katowice, all divisions already downloaded\n",
      "   Skipping 2025 Bilbao, all divisions already downloaded\n",
      "   Skipping 2025 Guadalajara, all divisions already downloaded\n",
      "   Skipping 2025 St. Gallen, all divisions already downloaded\n",
      "   Skipping 2025 Toulouse, all divisions already downloaded\n",
      "   Skipping 2025 Auckland, all divisions already downloaded\n",
      "   Skipping 2025 Las Vegas, all divisions already downloaded\n",
      "   Skipping 2025 Turin, all divisions already downloaded\n",
      "   Skipping 2025 Manchester, all divisions already downloaded\n",
      "   Skipping 2024 Frankfurt, all divisions already downloaded\n",
      "   Skipping 2024 Melbourne, all divisions already downloaded\n",
      "   Skipping 2024 Anaheim, all divisions already downloaded\n",
      "   Skipping 2024 Marseille, all divisions already downloaded\n",
      "   Skipping 2024 Stockholm, all divisions already downloaded\n",
      "   Skipping 2024 London, all divisions already downloaded\n",
      "   Skipping 2024 Hong Kong, all divisions already downloaded\n",
      "   Skipping 2024 Dallas, all divisions already downloaded\n",
      "   Skipping 2024 Beijing, all divisions already downloaded\n",
      "   Skipping 2024 Chicago Navy Pier, all divisions already downloaded\n",
      "   Skipping 2024 Dublin, all divisions already downloaded\n",
      "   Skipping 2024 Manchester, all divisions already downloaded\n",
      "   Skipping 2024 Ciudad de Mexico, all divisions already downloaded\n",
      "   Skipping 2024 Paris, all divisions already downloaded\n",
      "   Skipping 2024 Poznan, all divisions already downloaded\n",
      "   Skipping 2024 Hamburg, all divisions already downloaded\n",
      "   Skipping 2024 Madrid, all divisions already downloaded\n",
      "   Skipping 2024 Incheon, all divisions already downloaded\n",
      "   Skipping 2024 Milan, all divisions already downloaded\n",
      "   Skipping 2024 Amsterdam, all divisions already downloaded\n",
      "   Skipping 2024 Nice, all divisions already downloaded\n",
      "   Skipping 2024 Toronto, all divisions already downloaded\n",
      "   Skipping 2024 Stuttgart, all divisions already downloaded\n",
      "   Skipping 2024 Perth, all divisions already downloaded\n"
     ]
    },
    {
     "ename": "StaleElementReferenceException",
     "evalue": "Message: stale element reference: stale element not found in the current frame\n  (Session info: chrome=145.0.7632.76); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff66ae0aa55\n\t0x7ff66ab68630\n\t0x7ff66a8fd75d\n\t0x7ff66a9056c1\n\t0x7ff66a9087e4\n\t0x7ff66a9088af\n\t0x7ff66a9543b0\n\t0x7ff66a9819da\n\t0x7ff66a94aca6\n\t0x7ff66a9a591c\n\t0x7ff66a949098\n\t0x7ff66a949f83\n\t0x7ff66ae37810\n\t0x7ff66ae31afd\n\t0x7ff66ae52c1a\n\t0x7ff66ab83345\n\t0x7ff66ab8b81c\n\t0x7ff66ab71924\n\t0x7ff66ab71ad6\n\t0x7ff66ab57e47\n\t0x7ff8b1bae8d7\n\t0x7ff8b33cc40c\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 142\u001b[0m\n\u001b[0;32m    140\u001b[0m Select(driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault-lists-event_main_group\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mselect_by_value(race_value)\n\u001b[0;32m    141\u001b[0m human_pause(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m select_division \u001b[38;5;241m=\u001b[39m Select(driver\u001b[38;5;241m.\u001b[39mfind_element(By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault-lists-event\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    143\u001b[0m available_divisions \u001b[38;5;241m=\u001b[39m [o\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m select_division\u001b[38;5;241m.\u001b[39moptions]\n\u001b[0;32m    144\u001b[0m valid_divisions \u001b[38;5;241m=\u001b[39m [d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m DIVISION_OPTIONS \u001b[38;5;28;01mif\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m available_divisions]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\support\\select.py:41\u001b[0m, in \u001b[0;36mSelect.__init__\u001b[1;34m(self, webelement)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m UnexpectedTagNameException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelect only works on <select> elements, not on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwebelement\u001b[38;5;241m.\u001b[39mtag_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_el \u001b[38;5;241m=\u001b[39m webelement\n\u001b[1;32m---> 41\u001b[0m multi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_el\u001b[38;5;241m.\u001b[39mget_dom_attribute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiple\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_multiple \u001b[38;5;241m=\u001b[39m multi \u001b[38;5;129;01mand\u001b[39;00m multi \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webelement.py:182\u001b[0m, in \u001b[0;36mWebElement.get_dom_attribute\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dom_attribute\u001b[39m(\u001b[38;5;28mself\u001b[39m, name) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    168\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the HTML attribute value (not reflected properties) of the element.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[38;5;124;03m    Returns only attributes declared in the element's HTML markup, unlike\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m        text_length = target_element.get_dom_attribute(\"class\")\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute(Command\u001b[38;5;241m.\u001b[39mGET_ELEMENT_ATTRIBUTE, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: name})[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webelement.py:508\u001b[0m, in \u001b[0;36mWebElement._execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    506\u001b[0m     params \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    507\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id\n\u001b[1;32m--> 508\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent\u001b[38;5;241m.\u001b[39mexecute(command, params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:432\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    429\u001b[0m response \u001b[38;5;241m=\u001b[39m cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor)\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m--> 432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n\u001b[0;32m    433\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[1;34m(self, response)\u001b[0m\n\u001b[0;32m    230\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[1;31mStaleElementReferenceException\u001b[0m: Message: stale element reference: stale element not found in the current frame\n  (Session info: chrome=145.0.7632.76); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#staleelementreferenceexception\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff66ae0aa55\n\t0x7ff66ab68630\n\t0x7ff66a8fd75d\n\t0x7ff66a9056c1\n\t0x7ff66a9087e4\n\t0x7ff66a9088af\n\t0x7ff66a9543b0\n\t0x7ff66a9819da\n\t0x7ff66a94aca6\n\t0x7ff66a9a591c\n\t0x7ff66a949098\n\t0x7ff66a949f83\n\t0x7ff66ae37810\n\t0x7ff66ae31afd\n\t0x7ff66ae52c1a\n\t0x7ff66ab83345\n\t0x7ff66ab8b81c\n\t0x7ff66ab71924\n\t0x7ff66ab71ad6\n\t0x7ff66ab57e47\n\t0x7ff8b1bae8d7\n\t0x7ff8b33cc40c\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {\n",
    "    \"Season_7\": \"https://results.hyrox.com/season-7/?pid=start\",\n",
    "    \"Season_8\": \"https://results.hyrox.com/season-8/\"\n",
    "}\n",
    "\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "\n",
    "DIVISION_OPTIONS = [\n",
    "    \"HYROX PRO\",\n",
    "    \"HYROX PRO - Overall\",\n",
    "    \"HYROX PRO DOUBLES\",\n",
    "    \"HYROX PRO DOUBLES - Overall\"\n",
    "]\n",
    "\n",
    "def human_pause(a=1, b=3):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# DRIVER\n",
    "# ==========================\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE PAGES FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(race_name, division, gender, race_results):\n",
    "    page_number = 1\n",
    "    is_doubles = \"DOUBLES\" in division.upper()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")))\n",
    "        except TimeoutException:\n",
    "            print(\"      No results container found\")\n",
    "            return\n",
    "\n",
    "        human_pause(1, 2)\n",
    "\n",
    "        try:\n",
    "            container = driver.find_element(By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"      No container for this gender\")\n",
    "            return\n",
    "\n",
    "        rows = container.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "        if not rows:\n",
    "            print(\"      No rows found in container\")\n",
    "            return\n",
    "\n",
    "        for row in rows:\n",
    "            if \"list-group-header\" in row.get_attribute(\"class\"):\n",
    "                continue\n",
    "            try:\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\", \"\").strip()\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\", \"\").strip()\n",
    "\n",
    "                if is_doubles:\n",
    "                    members = row.find_elements(By.CSS_SELECTOR, \".type-relay_member a\")\n",
    "                    member_names = \" & \".join([m.text for m in members])\n",
    "                    race_results.append([race_name, division, gender, rank, age_rank, member_names, \"\", age_group, total_time])\n",
    "                else:\n",
    "                    name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "                    race_results.append([race_name, division, gender, rank, age_rank, name, nation, age_group, total_time])\n",
    "\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"      Page {page_number} scraped\")\n",
    "        human_pause(1, 2)\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            page_number += 1\n",
    "            human_pause(2, 4)\n",
    "        except NoSuchElementException:\n",
    "            print(\"      Last page reached\")\n",
    "            break\n",
    "\n",
    "# ==========================\n",
    "# MAIN LOOP\n",
    "# ==========================\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "    print(f\"\\n==============================\\nSTARTING {season}\\n==============================\\n\")\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    # ===== Build set of already downloaded CSVs =====\n",
    "    existing_csvs = set(os.listdir(season_folder))\n",
    "\n",
    "    driver.get(base_url)\n",
    "    human_pause(2, 4)\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\")))\n",
    "    except TimeoutException:\n",
    "        print(f\"No races found for {season}\")\n",
    "        continue\n",
    "\n",
    "    race_dropdown = Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "    races = [(opt.text, opt.get_attribute(\"value\")) for opt in race_dropdown.options]\n",
    "    print(f\"Found {len(races)} races in {season}\")\n",
    "\n",
    "    # ===== Build list of race/division combos that still need scraping =====\n",
    "    races_to_scrape = []\n",
    "    for race_name, race_value in races:\n",
    "        safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        driver.get(base_url)\n",
    "        human_pause(1, 2)\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "        human_pause(1, 2)\n",
    "        select_division = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "        available_divisions = [o.text for o in select_division.options]\n",
    "        valid_divisions = [d for d in DIVISION_OPTIONS if d in available_divisions]\n",
    "\n",
    "        # Check which divisions still need scraping\n",
    "        divisions_needed = [d for d in valid_divisions\n",
    "                            if f\"{safe_name}_{d.replace(' ', '_').replace('/', '-')}.csv\" not in existing_csvs]\n",
    "\n",
    "        if divisions_needed:\n",
    "            races_to_scrape.append((race_name, race_value, divisions_needed))\n",
    "        else:\n",
    "            print(f\"   Skipping {race_name}, all divisions already downloaded\")\n",
    "\n",
    "    print(f\"\\n{len(races_to_scrape)} races need scraping in {season}\")\n",
    "\n",
    "    # ===== Scrape only the races that need it =====\n",
    "    for race_name, race_value, divisions_needed in races_to_scrape:\n",
    "        safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        print(f\"\\nProcessing race: {race_name}\")\n",
    "\n",
    "        for div in divisions_needed:\n",
    "            div_safe_name = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "            file_path_div = os.path.join(season_folder, f\"{safe_name}_{div_safe_name}.csv\")\n",
    "            print(f\"   Division: {div}\")\n",
    "            division_results = []\n",
    "\n",
    "            for gender_code, gender_name in [(\"M\", \"Men\"), (\"W\", \"Women\")]:\n",
    "                print(f\"      Gender: {gender_name}\")\n",
    "\n",
    "                # Go back to base page and re-select race + division\n",
    "                driver.get(base_url)\n",
    "                human_pause(1, 2)\n",
    "                Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "                human_pause(1, 2)\n",
    "                Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "                human_pause(1, 2)\n",
    "\n",
    "                # Select gender and number of results\n",
    "                Select(driver.find_element(By.ID, \"default-lists-sex\")).select_by_value(gender_code)\n",
    "                Select(driver.find_element(By.ID, \"default-num_results\")).select_by_value(\"100\")\n",
    "                human_pause(1, 2)\n",
    "\n",
    "                # Submit form\n",
    "                driver.find_element(By.ID, \"default-submit\").click()\n",
    "                human_pause(2, 4)\n",
    "\n",
    "                # Scrape results\n",
    "                scrape_pages(race_name, div, gender_name, division_results)\n",
    "                human_pause(1, 2)\n",
    "\n",
    "            # Save CSV after both genders are scraped\n",
    "            columns = [\"Race\", \"Division\", \"Gender\", \"Rank Overall\", \"Rank Age Group\",\n",
    "                       \"Name\", \"Nation\", \"Age Group\", \"Total Time\"]\n",
    "            df_div = pd.DataFrame(division_results, columns=columns)\n",
    "            df_div.to_csv(file_path_div, index=False)\n",
    "            print(f\"   Saved {len(df_div)} rows to {file_path_div}\")\n",
    "            human_pause(1, 2)\n",
    "\n",
    "print(\"\\nALL DONE.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "60d2ea1a-baf5-4633-b14f-a58da7b99e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STARTING Season_7\n",
      "==============================\n",
      "\n",
      "Found 73 races in Season_7\n",
      "   Skipping 2025 Shanghai, already has 2 CSV files\n",
      "   Skipping 2025 Atlanta, already has 2 CSV files\n",
      "   Skipping 2025 Valencia, already has 2 CSV files\n",
      "   Skipping 2025 Maastricht, already has 2 CSV files\n",
      "   Skipping 2025 Mumbai, already has 2 CSV files\n",
      "   Skipping 2025 World Championships, already has 2 CSV files\n",
      "   Skipping 2025 Cardiff, already has 2 CSV files\n",
      "   Skipping 2025 New York, already has 2 CSV files\n",
      "   Skipping 2025 Riga, already has 2 CSV files\n",
      "\n",
      "Processing race: 2025 Rimini\n",
      "   Division: HYROX PRO - Friday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "   Saved 151 rows to Datasets\\Hyrox\\Season_7\\2025_Rimini_HYROX_PRO_-_Friday.csv\n",
      "   Division: HYROX PRO - Saturday\n",
      "      Gender: Men\n",
      "      Gender: Women\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_7\\2025_Rimini_HYROX_PRO_-_Saturday.csv\n",
      "\n",
      "Processing race: 2025 Bangkok\n",
      "   Division: HYROX PRO - Saturday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "   Saved 223 rows to Datasets\\Hyrox\\Season_7\\2025_Bangkok_HYROX_PRO_-_Saturday.csv\n",
      "   Division: HYROX - Saturday\n",
      "      Gender: Men\n",
      "      Gender: Women\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_7\\2025_Bangkok_HYROX_-_Saturday.csv\n",
      "\n",
      "Processing race: 2025 Berlin\n",
      "   Division: HYROX PRO - Friday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "   Saved 196 rows to Datasets\\Hyrox\\Season_7\\2025_Berlin_HYROX_PRO_-_Friday.csv\n",
      "   Division: HYROX PRO - Saturday\n",
      "      Gender: Men\n",
      "      Gender: Women\n",
      "   Saved 0 rows to Datasets\\Hyrox\\Season_7\\2025_Berlin_HYROX_PRO_-_Saturday.csv\n",
      "   Skipping 2025 Incheon, already has 2 CSV files\n",
      "\n",
      "Processing race: 2025 Heerenveen\n",
      "   Division: HYROX PRO - Saturday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "   Saved 215 rows to Datasets\\Hyrox\\Season_7\\2025_Heerenveen_HYROX_PRO_-_Saturday.csv\n",
      "   Division: HYROX - Friday\n",
      "      Gender: Men\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 200\u001b[0m\n\u001b[0;32m    193\u001b[0m     Select(driver\u001b[38;5;241m.\u001b[39mfind_element(\n\u001b[0;32m    194\u001b[0m         By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault-num_results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    195\u001b[0m     )\u001b[38;5;241m.\u001b[39mselect_by_value(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m100\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m     driver\u001b[38;5;241m.\u001b[39mfind_element(\n\u001b[0;32m    198\u001b[0m         By\u001b[38;5;241m.\u001b[39mID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault-submit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mclick()\n\u001b[1;32m--> 200\u001b[0m     human_pause(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m    202\u001b[0m     scrape_pages(race_name, div, gender_name, division_results)\n\u001b[0;32m    204\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(division_results, columns\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDivision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGender\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank Overall\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank Age Group\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge Group\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal Time\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m ])\n",
      "Cell \u001b[1;32mIn[47], line 24\u001b[0m, in \u001b[0;36mhuman_pause\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhuman_pause\u001b[39m(a\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m---> 24\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(random\u001b[38;5;241m.\u001b[39muniform(a, b))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {\n",
    "    \"Season_7\": \"https://results.hyrox.com/season-7/?pid=start\",\n",
    "    \"Season_8\": \"https://results.hyrox.com/season-8/\"\n",
    "}\n",
    "\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "\n",
    "def human_pause(a=1, b=2):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# DRIVER\n",
    "# ==========================\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 20)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPER FUNCTION\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(race_name, division, gender, race_results):\n",
    "    page_number = 1\n",
    "    is_doubles = \"DOUBLES\" in division.upper()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located(\n",
    "                (By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")))\n",
    "        except TimeoutException:\n",
    "            return\n",
    "\n",
    "        container = driver.find_element(\n",
    "            By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")\n",
    "\n",
    "        rows = container.find_elements(\n",
    "            By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "\n",
    "        for row in rows:\n",
    "            if \"list-group-header\" in row.get_attribute(\"class\"):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\", \"\").strip()\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\", \"\").strip()\n",
    "\n",
    "                if is_doubles:\n",
    "                    members = row.find_elements(By.CSS_SELECTOR, \".type-relay_member a\")\n",
    "                    name = \" & \".join([m.text for m in members])\n",
    "                    nation = \"\"\n",
    "                else:\n",
    "                    name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "\n",
    "                race_results.append([\n",
    "                    race_name, division, gender,\n",
    "                    rank, age_rank, name, nation,\n",
    "                    age_group, total_time\n",
    "                ])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"      Page {page_number} scraped\")\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            page_number += 1\n",
    "            human_pause(1, 2)\n",
    "        except NoSuchElementException:\n",
    "            print(\"      Last page reached\")\n",
    "            break\n",
    "\n",
    "# ==========================\n",
    "# MAIN LOOP\n",
    "# ==========================\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"STARTING {season}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    existing_files = os.listdir(season_folder)\n",
    "\n",
    "    driver.get(base_url)\n",
    "    human_pause(2, 3)\n",
    "\n",
    "    wait.until(EC.presence_of_element_located(\n",
    "        (By.ID, \"default-lists-event_main_group\")))\n",
    "\n",
    "    race_dropdown = Select(\n",
    "        driver.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "\n",
    "    races = [(opt.text, opt.get_attribute(\"value\"))\n",
    "             for opt in race_dropdown.options]\n",
    "\n",
    "    print(f\"Found {len(races)} races in {season}\")\n",
    "\n",
    "    for race_name, race_value in races:\n",
    "\n",
    "        safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "        # FAST CHECK: count how many CSVs start with race name\n",
    "        matching_files = [\n",
    "            f for f in existing_files\n",
    "            if f.startswith(safe_name + \"_\") and f.endswith(\".csv\")\n",
    "        ]\n",
    "\n",
    "        if len(matching_files) >= 2:\n",
    "            print(f\"   Skipping {race_name}, already has 2 CSV files\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing race: {race_name}\")\n",
    "\n",
    "        # Now open race page only if needed\n",
    "        driver.get(base_url)\n",
    "        human_pause(1, 2)\n",
    "\n",
    "        Select(driver.find_element(\n",
    "            By.ID, \"default-lists-event_main_group\")\n",
    "        ).select_by_value(race_value)\n",
    "\n",
    "        human_pause(1, 2)\n",
    "\n",
    "        select_division = Select(\n",
    "            driver.find_element(By.ID, \"default-lists-event\"))\n",
    "\n",
    "        available_divisions = [\n",
    "            o.text.strip()\n",
    "            for o in select_division.options\n",
    "            if o.text.strip() != \"\"\n",
    "        ]\n",
    "\n",
    "        # Only take first 2 divisions\n",
    "        divisions_to_scrape = available_divisions[:2]\n",
    "\n",
    "        for div in divisions_to_scrape:\n",
    "\n",
    "            div_safe = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "            file_path = os.path.join(\n",
    "                season_folder, f\"{safe_name}_{div_safe}.csv\")\n",
    "\n",
    "            if os.path.exists(file_path):\n",
    "                print(f\"   {div} already exists, skipping\")\n",
    "                continue\n",
    "\n",
    "            print(f\"   Division: {div}\")\n",
    "\n",
    "            division_results = []\n",
    "\n",
    "            for gender_code, gender_name in [(\"M\", \"Men\"), (\"W\", \"Women\")]:\n",
    "\n",
    "                print(f\"      Gender: {gender_name}\")\n",
    "\n",
    "                driver.get(base_url)\n",
    "                human_pause(1, 2)\n",
    "\n",
    "                Select(driver.find_element(\n",
    "                    By.ID, \"default-lists-event_main_group\")\n",
    "                ).select_by_value(race_value)\n",
    "\n",
    "                Select(driver.find_element(\n",
    "                    By.ID, \"default-lists-event\")\n",
    "                ).select_by_visible_text(div)\n",
    "\n",
    "                Select(driver.find_element(\n",
    "                    By.ID, \"default-lists-sex\")\n",
    "                ).select_by_value(gender_code)\n",
    "\n",
    "                Select(driver.find_element(\n",
    "                    By.ID, \"default-num_results\")\n",
    "                ).select_by_value(\"100\")\n",
    "\n",
    "                driver.find_element(\n",
    "                    By.ID, \"default-submit\").click()\n",
    "\n",
    "                human_pause(2, 3)\n",
    "\n",
    "                scrape_pages(race_name, div, gender_name, division_results)\n",
    "\n",
    "            df = pd.DataFrame(division_results, columns=[\n",
    "                \"Race\", \"Division\", \"Gender\",\n",
    "                \"Rank Overall\", \"Rank Age Group\",\n",
    "                \"Name\", \"Nation\", \"Age Group\", \"Total Time\"\n",
    "            ])\n",
    "\n",
    "            df.to_csv(file_path, index=False)\n",
    "            print(f\"   Saved {len(df)} rows to {file_path}\")\n",
    "\n",
    "print(\"\\nALL DONE.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7909c28a-a96f-49aa-8944-fb7da5acf115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "STARTING Season_7\n",
      "==============================\n",
      "\n",
      "Found 73 races in Season_7\n",
      "Skipping 2025 Shanghai, already downloaded\n",
      "Skipping 2025 Atlanta, already downloaded\n",
      "\n",
      "Processing race: 2025 Valencia\n",
      "   Division already downloaded: HYROX PRO - Overall\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   No rows scraped for this division\n",
      "Skipping 2025 Maastricht, already downloaded\n",
      "Skipping 2025 Mumbai, already downloaded\n",
      "\n",
      "Processing race: 2025 World Championships\n",
      "   Division: HYROX PRO - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   No rows scraped for this division\n",
      "   Division: HYROX PRO DOUBLES - Overall\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   No rows scraped for this division\n",
      "Skipping 2025 Cardiff, already downloaded\n",
      "Skipping 2025 New York, already downloaded\n",
      "Skipping 2025 Riga, already downloaded\n",
      "Skipping 2025 Rimini, already downloaded\n",
      "\n",
      "Processing race: 2025 Bangkok\n",
      "   Division already downloaded: HYROX PRO - Saturday\n",
      "   Division: HYROX PRO DOUBLES - Saturday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Last page reached\n",
      "   Saved 297 rows to Datasets\\Hyrox\\Season_7\\2025_Bangkok_HYROX_PRO_DOUBLES_-_Saturday.csv\n",
      "Skipping 2025 Berlin, already downloaded\n",
      "Skipping 2025 Incheon, already downloaded\n",
      "Skipping 2025 Heerenveen, already downloaded\n",
      "Skipping 2025 London, already downloaded\n",
      "\n",
      "Processing race: 2025 Barcelona\n",
      "   Division: HYROX PRO - Saturday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Page 3 scraped\n",
      "      Page 4 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "   Saved 500 rows to Datasets\\Hyrox\\Season_7\\2025_Barcelona_HYROX_PRO_-_Saturday.csv\n",
      "   Division: HYROX PRO DOUBLES - Saturday\n",
      "      Gender: Men\n",
      "      Page 1 scraped\n",
      "      Page 2 scraped\n",
      "      Last page reached\n",
      "      Gender: Women\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "# ==========================\n",
    "# CONFIG\n",
    "# ==========================\n",
    "\n",
    "BASE_URLS = {\n",
    "    \"Season_7\": \"https://results.hyrox.com/season-7/?pid=start\",\n",
    "    \"Season_8\": \"https://results.hyrox.com/season-8/\"\n",
    "}\n",
    "\n",
    "SAVE_ROOT = r\"Datasets\\Hyrox\"\n",
    "\n",
    "def human_pause(a=3, b=7):\n",
    "    time.sleep(random.uniform(a, b))\n",
    "\n",
    "# ==========================\n",
    "# DRIVER\n",
    "# ==========================\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\n",
    "    \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "    \"Chrome/120.0.0.0 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "wait = WebDriverWait(driver, 25)\n",
    "\n",
    "# ==========================\n",
    "# SCRAPE PAGES FUNCTION (UNCHANGED)\n",
    "# ==========================\n",
    "\n",
    "def scrape_pages(race_name, division, gender, race_results):\n",
    "    page_number = 1\n",
    "    is_doubles = \"DOUBLES\" in division.upper()\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")))\n",
    "        except TimeoutException:\n",
    "            print(\"      No results container found\")\n",
    "            return\n",
    "\n",
    "        human_pause(2, 5)\n",
    "\n",
    "        try:\n",
    "            container = driver.find_element(By.CSS_SELECTOR, f\"div[data-sex='{gender[0]}']\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"      No container for this gender\")\n",
    "            return\n",
    "\n",
    "        rows = container.find_elements(By.CSS_SELECTOR, \"li.list-group-item.row\")\n",
    "        if not rows:\n",
    "            print(\"      No rows found in container\")\n",
    "            return\n",
    "\n",
    "        for row in rows:\n",
    "            if \"list-group-header\" in row.get_attribute(\"class\"):\n",
    "                continue\n",
    "            try:\n",
    "                rank = row.find_element(By.CSS_SELECTOR, \".place-primary\").text\n",
    "                age_rank = row.find_element(By.CSS_SELECTOR, \".place-secondary\").text\n",
    "                total_time = row.find_element(By.CSS_SELECTOR, \".type-time\").text.replace(\"Total\", \"\").strip()\n",
    "                age_group = row.find_element(By.CSS_SELECTOR, \".type-age_class\").text.replace(\"Age Group\", \"\").strip()\n",
    "\n",
    "                if is_doubles:\n",
    "                    members = row.find_elements(By.CSS_SELECTOR, \".type-relay_member a\")\n",
    "                    member_names = \" & \".join([m.text for m in members])\n",
    "                    race_results.append([race_name, division, gender, rank, age_rank, member_names, \"\", age_group, total_time])\n",
    "                else:\n",
    "                    name = row.find_element(By.CSS_SELECTOR, \"h4.type-fullname\").text\n",
    "                    nation = row.find_element(By.CSS_SELECTOR, \".nation__abbr\").text\n",
    "                    race_results.append([race_name, division, gender, rank, age_rank, name, nation, age_group, total_time])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "        print(f\"      Page {page_number} scraped\")\n",
    "        human_pause(3, 6)\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, \"//a[text()='>']\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            page_number += 1\n",
    "            human_pause(5, 10)\n",
    "        except NoSuchElementException:\n",
    "            print(\"      Last page reached\")\n",
    "            break\n",
    "\n",
    "# ==========================\n",
    "# MAIN LOOP\n",
    "# ==========================\n",
    "\n",
    "for season, base_url in BASE_URLS.items():\n",
    "    print(f\"\\n==============================\\nSTARTING {season}\\n==============================\\n\")\n",
    "\n",
    "    season_folder = os.path.join(SAVE_ROOT, season)\n",
    "    os.makedirs(season_folder, exist_ok=True)\n",
    "\n",
    "    driver.get(base_url)\n",
    "    human_pause(5, 10)\n",
    "\n",
    "    try:\n",
    "        wait.until(EC.presence_of_element_located((By.ID, \"default-lists-event_main_group\")))\n",
    "    except TimeoutException:\n",
    "        print(f\"No races found for {season}\")\n",
    "        continue\n",
    "\n",
    "    race_dropdown = Select(driver.find_element(By.ID, \"default-lists-event_main_group\"))\n",
    "    races = [(opt.text, opt.get_attribute(\"value\")) for opt in race_dropdown.options]\n",
    "\n",
    "    print(f\"Found {len(races)} races in {season}\")\n",
    "\n",
    "    for race_name, race_value in races:\n",
    "\n",
    "        safe_name = race_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "\n",
    "        # ==========================\n",
    "        #  FAST RACE SKIP\n",
    "        # If 2 CSVs start with race name  skip race\n",
    "        # ==========================\n",
    "        existing_files = [f for f in os.listdir(season_folder) if f.startswith(safe_name)]\n",
    "        if len(existing_files) >= 2:\n",
    "            print(f\"Skipping {race_name}, already downloaded\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing race: {race_name}\")\n",
    "\n",
    "        # Load race once to inspect divisions\n",
    "        driver.get(base_url)\n",
    "        human_pause(4, 8)\n",
    "\n",
    "        Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "        human_pause(3, 6)\n",
    "\n",
    "        select_division = Select(driver.find_element(By.ID, \"default-lists-event\"))\n",
    "        available_divisions = [o.text.strip() for o in select_division.options]\n",
    "\n",
    "        divisions_to_scrape = []\n",
    "\n",
    "        # ==========================\n",
    "        # PRO LOGIC\n",
    "        # ==========================\n",
    "        if \"HYROX PRO\" in available_divisions:\n",
    "            divisions_to_scrape.append(\"HYROX PRO\")\n",
    "        elif \"HYROX PRO - Overall\" in available_divisions:\n",
    "            divisions_to_scrape.append(\"HYROX PRO - Overall\")\n",
    "        else:\n",
    "            pro_variants = [d for d in available_divisions if d.startswith(\"HYROX PRO -\")]\n",
    "            divisions_to_scrape.extend(pro_variants)\n",
    "\n",
    "        # ==========================\n",
    "        # DOUBLES LOGIC\n",
    "        # ==========================\n",
    "        if \"HYROX PRO DOUBLES\" in available_divisions:\n",
    "            divisions_to_scrape.append(\"HYROX PRO DOUBLES\")\n",
    "        elif \"HYROX PRO DOUBLES - Overall\" in available_divisions:\n",
    "            divisions_to_scrape.append(\"HYROX PRO DOUBLES - Overall\")\n",
    "        else:\n",
    "            doubles_variants = [d for d in available_divisions if d.startswith(\"HYROX PRO DOUBLES -\")]\n",
    "            divisions_to_scrape.extend(doubles_variants)\n",
    "\n",
    "        if not divisions_to_scrape:\n",
    "            print(\"   No valid HYROX PRO divisions found\")\n",
    "            continue\n",
    "\n",
    "        # ==========================\n",
    "        # LOOP DIVISIONS (UNCHANGED STRUCTURE)\n",
    "        # ==========================\n",
    "        for div in divisions_to_scrape:\n",
    "\n",
    "            div_safe_name = div.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "            file_path_div = os.path.join(season_folder, f\"{safe_name}_{div_safe_name}.csv\")\n",
    "\n",
    "            if os.path.exists(file_path_div):\n",
    "                print(f\"   Division already downloaded: {div}\")\n",
    "                continue\n",
    "\n",
    "            print(f\"   Division: {div}\")\n",
    "            division_results = []\n",
    "\n",
    "            for gender_code, gender_name in [(\"M\", \"Men\"), (\"W\", \"Women\")]:\n",
    "\n",
    "                print(f\"      Gender: {gender_name}\")\n",
    "\n",
    "                driver.get(base_url)\n",
    "                human_pause(4, 8)\n",
    "\n",
    "                Select(driver.find_element(By.ID, \"default-lists-event_main_group\")).select_by_value(race_value)\n",
    "                human_pause(2, 5)\n",
    "\n",
    "                Select(driver.find_element(By.ID, \"default-lists-event\")).select_by_visible_text(div)\n",
    "                human_pause(2, 5)\n",
    "\n",
    "                Select(driver.find_element(By.ID, \"default-lists-sex\")).select_by_value(gender_code)\n",
    "                Select(driver.find_element(By.ID, \"default-num_results\")).select_by_value(\"100\")\n",
    "                human_pause(2, 5)\n",
    "\n",
    "                driver.find_element(By.ID, \"default-submit\").click()\n",
    "                human_pause(5, 10)\n",
    "\n",
    "                scrape_pages(race_name, div, gender_name, division_results)\n",
    "                human_pause(2, 5)\n",
    "\n",
    "            columns = [\"Race\", \"Division\", \"Gender\", \"Rank Overall\", \"Rank Age Group\",\n",
    "                       \"Name\", \"Nation\", \"Age Group\", \"Total Time\"]\n",
    "\n",
    "            df_div = pd.DataFrame(division_results, columns=columns)\n",
    "\n",
    "            if len(df_div) > 0:\n",
    "                df_div.to_csv(file_path_div, index=False)\n",
    "                print(f\"   Saved {len(df_div)} rows to {file_path_div}\")\n",
    "            else:\n",
    "                print(\"   No rows scraped for this division\")\n",
    "\n",
    "            human_pause(3, 6)\n",
    "\n",
    "print(\"\\nALL DONE.\")\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2eccf8-54f5-4410-bb93-df0652da8cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
