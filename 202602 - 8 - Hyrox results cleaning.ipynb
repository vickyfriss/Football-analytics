{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f59313c2-033a-4998-9657-d7477e36e373",
   "metadata": {},
   "source": [
    "# Read all HYROX results CSV files, combining DOUBLES and SINGLES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f7f4f71-ee74-4531-8cd8-7676972b406b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DIVISION DETECTION SUMMARY =====\n",
      "\n",
      "\n",
      "ADAPTIVE (1)\n",
      "----------------------------------------\n",
      "HYROX_ADAPTIVE\n",
      "\n",
      "DOUBLES (4)\n",
      "----------------------------------------\n",
      "HYROX_DOUBLES\n",
      "HYROX_ELITE_15_DOUBLES\n",
      "HYROX_GORUCK_DOUBLES\n",
      "HYROX_PRO_DOUBLES\n",
      "\n",
      "ELITE (2)\n",
      "----------------------------------------\n",
      "HYROX_ELITE\n",
      "HYROX_ELITE_15\n",
      "\n",
      "GORUCK (1)\n",
      "----------------------------------------\n",
      "HYROX_GORUCK\n",
      "\n",
      "OPEN (SINGLES) (1)\n",
      "----------------------------------------\n",
      "HYROX\n",
      "\n",
      "OTHER / SPECIAL (1)\n",
      "----------------------------------------\n",
      "HYROX_MIXED_ASIEN_CHAMPIONSHIP_INVITATIONAL\n",
      "\n",
      "PRO (SINGLES) (1)\n",
      "----------------------------------------\n",
      "HYROX_PRO\n",
      "\n",
      "RELAY / TEAM (4)\n",
      "----------------------------------------\n",
      "HYROX_CORPORATE_RELAY\n",
      "HYROX_SECONDARY_SCHOOL_RELAY\n",
      "HYROX_TEAM_CHALLENGE\n",
      "HYROX_TEAM_RELAY\n",
      "\n",
      "YOUTH (1)\n",
      "----------------------------------------\n",
      "HYROX_YOUNGSTARS_8_9_YRS\n",
      "\n",
      "Total cleaned divisions: 16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "root_folder = r\"Datasets\\Hyrox\"\n",
    "\n",
    "raw_divisions = set()\n",
    "\n",
    "suffix_words = {\n",
    "    \"overall\", \"saturday\", \"sunday\", \"friday\",\n",
    "    \"thursday\", \"wednesday\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Extract divisions\n",
    "# -----------------------------\n",
    "for subdir, _, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if not file.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        name = file.replace(\".csv\", \"\")\n",
    "\n",
    "        if name.lower() in [\"doubles\", \"singles\", \"hyrox_pro\"]:\n",
    "            continue\n",
    "\n",
    "        parts = name.split(\"_\")\n",
    "\n",
    "        # Find first HYROX\n",
    "        hyrox_index = None\n",
    "        for i, p in enumerate(parts):\n",
    "            if p.lower() == \"hyrox\":\n",
    "                hyrox_index = i\n",
    "                break\n",
    "\n",
    "        if hyrox_index is None:\n",
    "            continue\n",
    "\n",
    "        division_parts = parts[hyrox_index:]\n",
    "\n",
    "        # Remove trailing weekday/overall\n",
    "        while division_parts and division_parts[-1].lower() in suffix_words:\n",
    "            division_parts = division_parts[:-1]\n",
    "\n",
    "        division = \"_\".join(division_parts)\n",
    "        raw_divisions.add(division)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Clean divisions\n",
    "# -----------------------------\n",
    "clean_divisions = set()\n",
    "\n",
    "for d in raw_divisions:\n",
    "    d = d.upper()\n",
    "    d = d.replace(\"-\", \"_\")\n",
    "    d = re.sub(r\"_+$\", \"\", d)\n",
    "    d = re.sub(r\"_+\", \"_\", d)\n",
    "    clean_divisions.add(d)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Detect categories\n",
    "# -----------------------------\n",
    "categories = defaultdict(list)\n",
    "\n",
    "for d in clean_divisions:\n",
    "\n",
    "    if \"DOUBLES\" in d:\n",
    "        categories[\"DOUBLES\"].append(d)\n",
    "\n",
    "    elif \"RELAY\" in d or \"TEAM\" in d:\n",
    "        categories[\"RELAY / TEAM\"].append(d)\n",
    "\n",
    "    elif \"GORUCK\" in d:\n",
    "        categories[\"GORUCK\"].append(d)\n",
    "\n",
    "    elif \"ADAPTIVE\" in d:\n",
    "        categories[\"ADAPTIVE\"].append(d)\n",
    "\n",
    "    elif \"YOUNGSTARS\" in d:\n",
    "        categories[\"YOUTH\"].append(d)\n",
    "\n",
    "    elif \"ELITE\" in d:\n",
    "        categories[\"ELITE\"].append(d)\n",
    "\n",
    "    elif \"PRO\" in d:\n",
    "        categories[\"PRO (SINGLES)\"].append(d)\n",
    "\n",
    "    elif d == \"HYROX\":\n",
    "        categories[\"OPEN (SINGLES)\"].append(d)\n",
    "\n",
    "    else:\n",
    "        categories[\"OTHER / SPECIAL\"].append(d)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Print results\n",
    "# -----------------------------\n",
    "print(\"\\n===== DIVISION DETECTION SUMMARY =====\\n\")\n",
    "\n",
    "for cat in sorted(categories.keys()):\n",
    "    print(f\"\\n{cat} ({len(categories[cat])})\")\n",
    "    print(\"-\" * 40)\n",
    "    for div in sorted(categories[cat]):\n",
    "        print(div)\n",
    "\n",
    "print(\"\\nTotal cleaned divisions:\", len(clean_divisions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c42c4ecc-b8ec-40ff-b845-6bb33b8c4a0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "EMPTY FILES SUMMARY\n",
      "============================\n",
      "Total empty files: 148\n",
      "\n",
      "- 2019_Hannover_HYROX_TEAM-CHALLENGE\n",
      "- 2021_Austin_HYROX_ELITE\n",
      "- 2021_New_York_Hyrox_Team_Relay\n",
      "- 2021_Orlando_HYROX_ELITE\n",
      "- 2022_Basel_HYROX_TEAM_RELAY\n",
      "- 2022_Birmingham_HYROX_TEAM_RELAY\n",
      "- 2022_Bremen_Hyrox_Team_Relay\n",
      "- 2022_Essen_HYROX_TEAM_RELAY\n",
      "- 2022_Essen_Hyrox_Team_Relay\n",
      "- 2022_Las_Vegas_HYROX_ELITE\n",
      "- 2022_Leipzig_HYROX_TEAM_RELAY\n",
      "- 2022_London_HYROX_TEAM_RELAY\n",
      "- 2022_Los_Angeles_HYROX_TEAM_RELAY\n",
      "- 2022_Maastricht_Hyrox_Team_Relay\n",
      "- 2022_Manchester_Hyrox_Team_Relay\n",
      "- 2022_New_York_HYROX_TEAM_RELAY\n",
      "- 2022_Wien_Hyrox_Team_Relay\n",
      "- 2023_Amsterdam_HYROX_TEAM_RELAY\n",
      "- 2023_Barcelona_HYROX_TEAM_RELAY\n",
      "- 2023_Chicago_HYROX_ELITE\n",
      "- 2023_Chicago_HYROX_TEAM_RELAY\n",
      "- 2023_Dallas_HYROX_GORUCK\n",
      "- 2023_Dallas_HYROX_GORUCK_DOUBLES\n",
      "- 2023_Dallas_HYROX_TEAM_RELAY\n",
      "- 2023_Dallas_HYROX_TEAM_RELAY\n",
      "- 2023_Dublin_HYROX_TEAM_RELAY\n",
      "- 2023_Glasgow_HYROX_TEAM_RELAY\n",
      "- 2023_Hamburg_HYROX_TEAM_RELAY\n",
      "- 2023_Hong_Kong_HYROX_TEAM_RELAY\n",
      "- 2023_Köln_HYROX_TEAM_RELAY\n",
      "- 2023_London_HYROX_TEAM_RELAY\n",
      "- 2023_Los_Angeles_HYROX_TEAM_RELAY\n",
      "- 2023_Maastricht_European_Championships_HYROX_ELITE\n",
      "- 2023_Maastricht_European_Championships_HYROX_TEAM_RELAY\n",
      "- 2023_Manchester_HYROX_TEAM_RELAY\n",
      "- 2023_Melbourne_HYROX_TEAM_RELAY\n",
      "- 2023_New_York_HYROX_TEAM_RELAY\n",
      "- 2023_Paris_HYROX_TEAM_RELAY\n",
      "- 2023_Stockholm_HYROX_ELITE\n",
      "- 2023_Stockholm_HYROX_TEAM_RELAY\n",
      "- 2023_Stuttgart_HYROX_TEAM_RELAY\n",
      "- 2023_Sydney_HYROX_TEAM_RELAY\n",
      "- 2024_Amsterdam_HYROX_ELITE_-_Thursday\n",
      "- 2024_Anaheim_HYROX_ADAPTIVE\n",
      "- 2024_Anaheim_HYROX_TEAM_RELAY\n",
      "- 2024_Cape_Town_HYROX_TEAM_RELAY\n",
      "- 2024_Ciudad_de_Mexico_HYROX_TEAM_RELAY\n",
      "- 2024_Dallas_HYROX_ADAPTIVE\n",
      "- 2024_Dallas_HYROX_TEAM_RELAY\n",
      "- 2024_Doha_All_Women's_Race_HYROX_PRO\n",
      "- 2024_Doha_All_Women's_Race_HYROX_TEAM_RELAY\n",
      "- 2024_Hong_Kong_HYROX_CORPORATE_RELAY\n",
      "- 2024_Hong_Kong_HYROX_ELITE\n",
      "- 2024_Hong_Kong_HYROX_MIXED_ASIEN_CHAMPIONSHIP_INVITATIONAL\n",
      "- 2024_Hong_Kong_HYROX_SECONDARY_SCHOOL_RELAY\n",
      "- 2024_Hong_Kong_HYROX_TEAM_RELAY\n",
      "- 2024_Karlsruhe_HYROX_TEAM_RELAY\n",
      "- 2024_Manchester_HYROX_TEAM_RELAY\n",
      "- 2024_Marseille_HYROX_TEAM_RELAY\n",
      "- 2024_Mexico_City_HYROX_TEAM_RELAY\n",
      "- 2024_Nice_HYROX_TEAM_RELAY\n",
      "- 2024_Vienna_-_European_Championship_HYROX_ELITE\n",
      "- 2025_Abu_Dhabi_HYROX\n",
      "- 2025_Abu_Dhabi_HYROX_ADAPTIVE\n",
      "- 2025_Abu_Dhabi_HYROX_DOUBLES\n",
      "- 2025_Abu_Dhabi_HYROX_PRO\n",
      "- 2025_Abu_Dhabi_HYROX_PRO_DOUBLES\n",
      "- 2025_Abu_Dhabi_HYROX_TEAM_RELAY\n",
      "- 2025_Acapulco_HYROX_TEAM_RELAY\n",
      "- 2025_Auckland_HYROX_TEAM_RELAY\n",
      "- 2025_Beijing_HYROX_ADAPTIVE\n",
      "- 2025_Beijing_HYROX_TEAM_RELAY\n",
      "- 2025_Bordeaux_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Boston_HYROX_-_Overall\n",
      "- 2025_Boston_HYROX_ADAPTIVE_-_Overall\n",
      "- 2025_Boston_HYROX_DOUBLES_-_Overall\n",
      "- 2025_Boston_HYROX_PRO_-_Overall\n",
      "- 2025_Boston_HYROX_PRO_DOUBLES_-_Overall\n",
      "- 2025_Boston_HYROX_TEAM_RELAY_-_Friday\n",
      "- 2025_Boston_HYROX_TEAM_RELAY_-_Overall\n",
      "- 2025_Brisbane_HYROX_TEAM_RELAY\n",
      "- 2025_Delhi_HYROX_ADAPTIVE\n",
      "- 2025_Delhi_HYROX_TEAM_RELAY\n",
      "- 2025_Gdansk_HYROX_ADAPTIVE\n",
      "- 2025_Gdansk_HYROX_TEAM_RELAY\n",
      "- 2025_Geneva_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Glasgow_HYROX_ELITE_-_Wednesday\n",
      "- 2025_Hamburg_HYROX_ELITE_15_-_Friday\n",
      "- 2025_Hamburg_HYROX_ELITE_15_DOUBLES_-_Saturday\n",
      "- 2025_Hong_Kong_HYROX_CORPORATE_RELAY_-_Saturday\n",
      "- 2025_Hong_Kong_HYROX_CORPORATE_RELAY_-_Sunday\n",
      "- 2025_Hong_Kong_HYROX_TEAM_RELAY_-_Saturday\n",
      "- 2025_Hong_Kong_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Las_Vegas_HYROX_ADAPTIVE_-_Overall\n",
      "- 2025_Las_Vegas_HYROX_ADAPTIVE_-_Saturday\n",
      "- 2025_Las_Vegas_HYROX_ADAPTIVE_-_Sunday\n",
      "- 2025_Las_Vegas_HYROX_ELITE_-_Friday\n",
      "- 2025_Las_Vegas_HYROX_TEAM_RELAY_-_Overall\n",
      "- 2025_Las_Vegas_HYROX_TEAM_RELAY_-_Saturday\n",
      "- 2025_Las_Vegas_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Melbourne_HYROX_ELITE_15_-_Friday\n",
      "- 2025_Melbourne_HYROX_ELITE_15_DOUBLES_-_Saturday\n",
      "- 2025_Mumbai_HYROX_TEAM_RELAY\n",
      "- 2025_Oslo_HYROX_ADAPTIVE_-_Friday\n",
      "- 2025_Oslo_HYROX_TEAM_RELAY_-_Overall\n",
      "- 2025_Oslo_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Rio_de_Janeiro_HYROX_PRO_DOUBLES\n",
      "- 2025_Sao_Paulo_HYROX_TEAM_RELAY\n",
      "- 2025_Seoul_HYROX_TEAM_RELAY\n",
      "- 2025_Singapore_Expo_HYROX_PRO_DOUBLES\n",
      "- 2025_Singapore_HYROX_CORPORATE_RELAY\n",
      "- 2025_Singapore_HYROX_DOUBLES\n",
      "- 2025_Singapore_HYROX_TEAM_RELAY\n",
      "- 2025_Sydney_HYROX_-_Overall\n",
      "- 2025_Sydney_HYROX_ADAPTIVE_-_Overall\n",
      "- 2025_Sydney_HYROX_TEAM_RELAY_-_Friday\n",
      "- 2025_Toronto_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Turin_HYROX\n",
      "- 2025_Turin_HYROX_PRO\n",
      "- 2025_Valencia_HYROX_-_Overall\n",
      "- 2025_Valencia_HYROX_ADAPTIVE_-_Friday\n",
      "- 2025_Valencia_HYROX_DOUBLES_-_Overall\n",
      "- 2025_Valencia_HYROX_PRO_DOUBLES_-_Overall\n",
      "- 2025_Valencia_HYROX_TEAM_RELAY_-_Overall\n",
      "- 2025_Valencia_HYROX_TEAM_RELAY_-_Overall\n",
      "- 2025_Valencia_HYROX_TEAM_RELAY_-_Saturday\n",
      "- 2025_Valencia_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_Valencia_HYROX_TEAM_RELAY_-_Sunday\n",
      "- 2025_World_Championships_HYROX_ELITE_15_-_Thursday\n",
      "- 2026_Auckland_HYROX_TEAM_RELAY_-_Thursday\n",
      "- 2026_Fortaleza_HYROX\n",
      "- 2026_Fortaleza_HYROX_ADAPTIVE\n",
      "- 2026_Fortaleza_HYROX_DOUBLES\n",
      "- 2026_Fortaleza_HYROX_PRO\n",
      "- 2026_Fortaleza_HYROX_PRO_DOUBLES\n",
      "- 2026_Fortaleza_HYROX_TEAM_RELAY\n",
      "- 2026_Phoenix_HYROX_ELITE_15_-_Thursday\n",
      "- 2026_Phoenix_HYROX_ELITE_15_DOUBLES_-_Friday\n",
      "- 2026_Washington_DC_HYROX_-_Overall\n",
      "- 2026_Washington_DC_HYROX_-_Saturday\n",
      "- 2026_Washington_DC_HYROX_-_Sunday\n",
      "- 2026_Washington_DC_HYROX_DOUBLES_-_Overall\n",
      "- 2026_Washington_DC_HYROX_DOUBLES_-_Saturday\n",
      "- 2026_Washington_DC_HYROX_DOUBLES_-_Sunday\n",
      "- 2026_Washington_DC_HYROX_ELITE_15_-_Saturday\n",
      "- 2026_Washington_DC_HYROX_ELITE_15_DOUBLES_-_Sunday\n",
      "- 2026_Washington_DC_HYROX_TEAM_RELAY_-_Sunday\n",
      "- WorldChampionship_-_Leipzig_HYROX_ELITE\n",
      "\n",
      "============================\n",
      "DATASETS CREATED\n",
      "============================\n",
      "HYROX → 298175 rows\n",
      "HYROX_ADAPTIVE → 208 rows\n",
      "HYROX_DOUBLES → 181104 rows\n",
      "HYROX_ELITE_15_DOUBLES → 29 rows\n",
      "HYROX_GORUCK → 274 rows\n",
      "HYROX_GORUCK_DOUBLES → 77 rows\n",
      "HYROX_PRO → 70688 rows\n",
      "HYROX_PRO_DOUBLES → 21354 rows\n",
      "HYROX_YOUNGSTARS_8_9_YRS → 377 rows\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "root_folder = r\"Datasets\\Hyrox\"\n",
    "output_folder = os.path.join(root_folder, \"Processed dataset\")\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "division_files = defaultdict(list)\n",
    "division_datasets = {}\n",
    "empty_files_detected = []\n",
    "\n",
    "suffix_words = {\n",
    "    \"overall\", \"saturday\", \"sunday\", \"friday\",\n",
    "    \"thursday\", \"wednesday\"\n",
    "}\n",
    "\n",
    "# ---------------------------------\n",
    "# STEP 1: Group files by division\n",
    "# ---------------------------------\n",
    "for subdir, _, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "\n",
    "        if not file.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "\n",
    "        if \"processed dataset\" in subdir.lower():\n",
    "            continue\n",
    "\n",
    "        filename_without_ext = file.replace(\".csv\", \"\")\n",
    "\n",
    "        if filename_without_ext.lower() in [\"doubles\", \"singles\", \"hyrox_pro\"]:\n",
    "            continue\n",
    "\n",
    "        parts = filename_without_ext.split(\"_\")\n",
    "\n",
    "        hyrox_index = None\n",
    "        for i, p in enumerate(parts):\n",
    "            if p.lower() == \"hyrox\":\n",
    "                hyrox_index = i\n",
    "                break\n",
    "\n",
    "        if hyrox_index is None:\n",
    "            continue\n",
    "\n",
    "        division_parts = parts[hyrox_index:]\n",
    "\n",
    "        while division_parts and division_parts[-1].lower() in suffix_words:\n",
    "            division_parts = division_parts[:-1]\n",
    "\n",
    "        division = \"_\".join(division_parts)\n",
    "\n",
    "        division = division.upper()\n",
    "        division = division.replace(\"-\", \"_\")\n",
    "        division = re.sub(r\"_+$\", \"\", division)\n",
    "        division = re.sub(r\"_+\", \"_\", division)\n",
    "\n",
    "        file_path = os.path.join(subdir, file)\n",
    "        division_files[division].append((file_path, filename_without_ext))\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# STEP 2: Create dataset per division\n",
    "# ---------------------------------\n",
    "for division, files in division_files.items():\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for file_path, filename_without_ext in files:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            if df.empty:\n",
    "                empty_files_detected.append(filename_without_ext)\n",
    "                continue\n",
    "\n",
    "            df[\"source_file\"] = filename_without_ext\n",
    "            dfs.append(df)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if dfs:\n",
    "        combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "        output_path = os.path.join(output_folder, f\"{division}.csv\")\n",
    "        combined_df.to_csv(output_path, index=False)\n",
    "\n",
    "        division_datasets[division] = combined_df\n",
    "        globals()[division] = combined_df\n",
    "\n",
    "\n",
    "# ---------------------------------\n",
    "# FINAL SUMMARY\n",
    "# ---------------------------------\n",
    "print(\"\\n============================\")\n",
    "print(\"EMPTY FILES SUMMARY\")\n",
    "print(\"============================\")\n",
    "print(f\"Total empty files: {len(empty_files_detected)}\\n\")\n",
    "\n",
    "if empty_files_detected:\n",
    "    for file in sorted(empty_files_detected):\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(\"No empty files detected.\")\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"DATASETS CREATED\")\n",
    "print(\"============================\")\n",
    "\n",
    "if division_datasets:\n",
    "    for name in sorted(division_datasets.keys()):\n",
    "        print(f\"{name} → {len(division_datasets[name])} rows\")\n",
    "else:\n",
    "    print(\"No datasets created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525dbe34-d0d4-4465-a9a3-1e1465eedaf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Race</th>\n",
       "      <th>Division</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Rank Overall</th>\n",
       "      <th>Rank Age Group</th>\n",
       "      <th>Name</th>\n",
       "      <th>Nation</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Total Time</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181099</th>\n",
       "      <td>2026 Auckland</td>\n",
       "      <td>HYROX DOUBLES - Thursday</td>\n",
       "      <td>Women</td>\n",
       "      <td>476</td>\n",
       "      <td>114</td>\n",
       "      <td>Clementine Smart, Samantha Collings</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30-34</td>\n",
       "      <td>02:26:31</td>\n",
       "      <td>2026_Auckland_HYROX_DOUBLES_-_Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181100</th>\n",
       "      <td>2026 Auckland</td>\n",
       "      <td>HYROX DOUBLES - Thursday</td>\n",
       "      <td>Women</td>\n",
       "      <td>477</td>\n",
       "      <td>115</td>\n",
       "      <td>Andrea Munokoatini, Jasmine Young</td>\n",
       "      <td>NaN</td>\n",
       "      <td>30-34</td>\n",
       "      <td>02:26:34</td>\n",
       "      <td>2026_Auckland_HYROX_DOUBLES_-_Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181101</th>\n",
       "      <td>2026 Auckland</td>\n",
       "      <td>HYROX DOUBLES - Thursday</td>\n",
       "      <td>Women</td>\n",
       "      <td>478</td>\n",
       "      <td>105</td>\n",
       "      <td>Holley Hamill, Janine Hamill</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35-39</td>\n",
       "      <td>02:29:25</td>\n",
       "      <td>2026_Auckland_HYROX_DOUBLES_-_Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181102</th>\n",
       "      <td>2026 Auckland</td>\n",
       "      <td>HYROX DOUBLES - Thursday</td>\n",
       "      <td>Women</td>\n",
       "      <td>479</td>\n",
       "      <td>26</td>\n",
       "      <td>Connie Valivaka, Santana Webster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45-49</td>\n",
       "      <td>02:36:10</td>\n",
       "      <td>2026_Auckland_HYROX_DOUBLES_-_Thursday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181103</th>\n",
       "      <td>2026 Auckland</td>\n",
       "      <td>HYROX DOUBLES - Thursday</td>\n",
       "      <td>Women</td>\n",
       "      <td>480</td>\n",
       "      <td>27</td>\n",
       "      <td>Charlene Rewiri-Ulufonua, Penni Gray</td>\n",
       "      <td>NaN</td>\n",
       "      <td>45-49</td>\n",
       "      <td>03:02:12</td>\n",
       "      <td>2026_Auckland_HYROX_DOUBLES_-_Thursday</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Race                  Division Gender  Rank Overall  \\\n",
       "181099  2026 Auckland  HYROX DOUBLES - Thursday  Women           476   \n",
       "181100  2026 Auckland  HYROX DOUBLES - Thursday  Women           477   \n",
       "181101  2026 Auckland  HYROX DOUBLES - Thursday  Women           478   \n",
       "181102  2026 Auckland  HYROX DOUBLES - Thursday  Women           479   \n",
       "181103  2026 Auckland  HYROX DOUBLES - Thursday  Women           480   \n",
       "\n",
       "       Rank Age Group                                  Name  Nation Age Group  \\\n",
       "181099            114   Clementine Smart, Samantha Collings     NaN     30-34   \n",
       "181100            115     Andrea Munokoatini, Jasmine Young     NaN     30-34   \n",
       "181101            105          Holley Hamill, Janine Hamill     NaN     35-39   \n",
       "181102             26      Connie Valivaka, Santana Webster     NaN     45-49   \n",
       "181103             27  Charlene Rewiri-Ulufonua, Penni Gray     NaN     45-49   \n",
       "\n",
       "       Total Time                             source_file  \n",
       "181099   02:26:31  2026_Auckland_HYROX_DOUBLES_-_Thursday  \n",
       "181100   02:26:34  2026_Auckland_HYROX_DOUBLES_-_Thursday  \n",
       "181101   02:29:25  2026_Auckland_HYROX_DOUBLES_-_Thursday  \n",
       "181102   02:36:10  2026_Auckland_HYROX_DOUBLES_-_Thursday  \n",
       "181103   03:02:12  2026_Auckland_HYROX_DOUBLES_-_Thursday  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HYROX_DOUBLES.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb404d7-ee99-4ae0-88a0-8cd14058e0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "GLOBAL DATA QUALITY SUMMARY (RAW)\n",
      "============================\n",
      "Total rows across all divisions: 572286\n",
      "Total duplicate rows detected: 12291\n",
      "Duplicate rows percentage: 2.15%\n",
      "\n",
      "----------------------------\n",
      "FILE-LEVEL QUALITY\n",
      "----------------------------\n",
      "Total files processed: 1088\n",
      "Empty files: 148\n",
      "Files with duplicate rows: 211\n",
      "Total files with issues (empty OR duplicates): 356\n",
      "Percentage of files with issues: 32.72%\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# GLOBAL DATA QUALITY SUMMARY (RAW DATA)\n",
    "# ---------------------------------\n",
    "\n",
    "total_rows_all_divisions = 0\n",
    "total_duplicate_rows_global = 0\n",
    "files_with_duplicates = set()\n",
    "\n",
    "for division, df in division_datasets.items():\n",
    "\n",
    "    total_rows_all_divisions += len(df)\n",
    "\n",
    "    duplicate_mask = df.duplicated(keep=False)\n",
    "\n",
    "    if duplicate_mask.any():\n",
    "\n",
    "        duplicates_df = df.loc[duplicate_mask]\n",
    "        total_duplicate_rows_global += len(duplicates_df)\n",
    "\n",
    "        dup_files = duplicates_df[\"source_file\"].unique()\n",
    "        files_with_duplicates.update(dup_files)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# FILE COUNTS\n",
    "# -------------------------\n",
    "\n",
    "all_files_processed = set()\n",
    "\n",
    "for division, df in division_datasets.items():\n",
    "    all_files_processed.update(df[\"source_file\"].unique())\n",
    "\n",
    "all_files_processed.update(empty_files_detected)\n",
    "\n",
    "total_files = len(all_files_processed)\n",
    "\n",
    "empty_files_count = len(empty_files_detected)\n",
    "duplicate_files_count = len(files_with_duplicates)\n",
    "\n",
    "files_with_issues = set(empty_files_detected).union(files_with_duplicates)\n",
    "total_files_with_issues = len(files_with_issues)\n",
    "\n",
    "# -------------------------\n",
    "# PRINT SUMMARY\n",
    "# -------------------------\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"GLOBAL DATA QUALITY SUMMARY (RAW)\")\n",
    "print(\"============================\")\n",
    "\n",
    "print(f\"Total rows across all divisions: {total_rows_all_divisions}\")\n",
    "print(f\"Total duplicate rows detected: {total_duplicate_rows_global}\")\n",
    "\n",
    "if total_rows_all_divisions > 0:\n",
    "    print(f\"Duplicate rows percentage: {(total_duplicate_rows_global / total_rows_all_divisions) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\n----------------------------\")\n",
    "print(\"FILE-LEVEL QUALITY\")\n",
    "print(\"----------------------------\")\n",
    "\n",
    "print(f\"Total files processed: {total_files}\")\n",
    "\n",
    "print(f\"Empty files: {empty_files_count}\")\n",
    "print(f\"Files with duplicate rows: {duplicate_files_count}\")\n",
    "print(f\"Total files with issues (empty OR duplicates): {total_files_with_issues}\")\n",
    "\n",
    "if total_files > 0:\n",
    "    print(f\"Percentage of files with issues: {(total_files_with_issues / total_files) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f0c37c-72d0-4b90-b173-f9e8861df407",
   "metadata": {},
   "source": [
    "# CHECK 1: Duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dfdc824-2223-4360-a467-71afcf49aac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for division, df in division_datasets.items():\n",
    "    df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a2d3c1-c7d0-4b38-9c09-8c6f5442a4ab",
   "metadata": {},
   "source": [
    "#### Notes:  \n",
    "* Cologne has all rows duplicated in the doubles (https://results.hyrox.com/season-7/?page=1&event=HDP_COLOGNE25_OVERALL&num_results=100&pid=list&pidp=ranking_nav&ranking=time_finish_netto&search%5Bsex%5D=M&search%5Bage_class%5D=%25&search%5Bnation%5D=%25)  \n",
    "* Same in other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006df7be-b17d-4d9f-a29a-4f027658e53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================\n",
      "GLOBAL DATA QUALITY SUMMARY\n",
      "============================\n",
      "Total rows across all divisions: 566108\n",
      "Total duplicate rows detected: 0\n",
      "Duplicate rows percentage: 0.00%\n",
      "\n",
      "Total files processed: 1088\n",
      "Files with issues (empty or duplicates): 145\n",
      "Percentage of files with issues: 13.33%\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------\n",
    "# GLOBAL DATA HEALTH SUMMARY\n",
    "# ---------------------------------\n",
    "\n",
    "total_rows_all_divisions = 0\n",
    "total_duplicate_rows = 0\n",
    "files_with_duplicates = set()\n",
    "\n",
    "for division, df in division_datasets.items():\n",
    "\n",
    "    # Count total rows AFTER cleaning\n",
    "    total_rows_all_divisions += len(df)\n",
    "\n",
    "    # Detect duplicates again on original logic (before drop)\n",
    "    duplicate_mask = df.duplicated(keep=False)\n",
    "\n",
    "    if duplicate_mask.any():\n",
    "        duplicates_df = df.loc[duplicate_mask]\n",
    "        total_duplicate_rows += len(duplicates_df)\n",
    "\n",
    "        dup_files = duplicates_df[\"source_file\"].unique()\n",
    "        files_with_duplicates.update(dup_files)\n",
    "\n",
    "# Total unique files processed\n",
    "all_files_processed = set()\n",
    "\n",
    "for division, df in division_datasets.items():\n",
    "    all_files_processed.update(df[\"source_file\"].unique())\n",
    "\n",
    "# Add empty files to total file count\n",
    "all_files_processed.update(empty_files_detected)\n",
    "\n",
    "total_files = len(all_files_processed)\n",
    "\n",
    "# Files with issues\n",
    "files_with_issues = set(empty_files_detected).union(files_with_duplicates)\n",
    "\n",
    "n_files_with_issues = len(files_with_issues)\n",
    "\n",
    "# Percentages\n",
    "if total_files > 0:\n",
    "    pct_files_with_issues = (n_files_with_issues / total_files) * 100\n",
    "else:\n",
    "    pct_files_with_issues = 0\n",
    "\n",
    "\n",
    "print(\"\\n============================\")\n",
    "print(\"GLOBAL DATA QUALITY SUMMARY\")\n",
    "print(\"============================\")\n",
    "\n",
    "print(f\"Total rows across all divisions: {total_rows_all_divisions}\")\n",
    "print(f\"Total duplicate rows detected: {total_duplicate_rows}\")\n",
    "\n",
    "if total_rows_all_divisions > 0:\n",
    "    print(f\"Duplicate rows percentage: {(total_duplicate_rows / total_rows_all_divisions) * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nTotal files processed:\", total_files)\n",
    "print(\"Files with issues (empty or duplicates):\", n_files_with_issues)\n",
    "print(f\"Percentage of files with issues: {pct_files_with_issues:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507ae17e-a128-4cd5-9349-49e354a5c1c2",
   "metadata": {},
   "source": [
    "# CHECK 2: Races where scraping only ran the first page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e912d96-9fe4-45e6-b603-a85bf84a70b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOUBLES dataset counts (exactly 100 rows for Race+Division+Gender):\n",
      "Race            Division  Gender\n",
      "2023 Stuttgart  HYROX     Women     100\n",
      "dtype: int64\n",
      "\n",
      "HYROX_pro dataset counts (exactly 100 rows for Race+Division+Gender):\n",
      "Race             Division   Gender\n",
      "2023 Manchester  HYROX PRO  Men       100\n",
      "2025 Manchester  HYROX PRO  Women     100\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check row counts per combination of Race, Division, and Gender in DOUBLES\n",
    "doubles_counts_gender = HYROX.groupby(['Race', 'Division', 'Gender']).size()\n",
    "doubles_100 = doubles_counts_gender[doubles_counts_gender == 100]\n",
    "print(\"DOUBLES dataset counts (exactly 100 rows for Race+Division+Gender):\")\n",
    "print(doubles_100)\n",
    "\n",
    "# Check row counts per combination of Race, Division, and Gender in HYROX_pro\n",
    "pro_counts_gender = HYROX_PRO.groupby(['Race', 'Division', 'Gender']).size()\n",
    "pro_100 = pro_counts_gender[pro_counts_gender == 100]\n",
    "print(\"\\nHYROX_pro dataset counts (exactly 100 rows for Race+Division+Gender):\")\n",
    "print(pro_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40ffd39-3c1d-4201-9b52-57e991dc4fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "root_folder = r\"Datasets\\Hyrox\"\n",
    "\n",
    "# Function to find and delete CSVs containing a specific combination\n",
    "def delete_csvs_with_combinations(df, combinations):\n",
    "    for subdir, _, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".csv\") and file not in [\"DOUBLES.csv\", \"HYROX_pro.csv\"]:\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                temp_df = pd.read_csv(file_path)\n",
    "                \n",
    "                # Check if any of the \"exactly 100 rows\" combinations are fully contained in this CSV\n",
    "                for comb in combinations.index:\n",
    "                    race, division, gender = comb\n",
    "                    matching_rows = temp_df[\n",
    "                        (temp_df['Race'] == race) &\n",
    "                        (temp_df['Division'] == division) &\n",
    "                        (temp_df['Gender'] == gender)\n",
    "                    ]\n",
    "                    if len(matching_rows) == 100:\n",
    "                        print(f\"Deleting {file_path} (matches {comb})\")\n",
    "                        os.remove(file_path)\n",
    "                        break  # move to next file once deleted\n",
    "\n",
    "# Delete for DOUBLES\n",
    "if not doubles_100.empty:\n",
    "    delete_csvs_with_combinations(DOUBLES_df, doubles_100)\n",
    "\n",
    "# Delete for HYROX_pro\n",
    "if not pro_100.empty:\n",
    "    delete_csvs_with_combinations(HYROX_pro_df, pro_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc5ba2-8bb6-44c3-9c60-ef5a8d32c90b",
   "metadata": {},
   "source": [
    "# CHECK 3: Empty datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7491413c-e921-4763-8399-42ff86741fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "root_folder = r\"Datasets\\Hyrox\"\n",
    "\n",
    "# Delete CSVs with only headers\n",
    "for subdir, _, files in os.walk(root_folder):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".csv\") and file not in [\"DOUBLES.csv\", \"HYROX_pro.csv\"]:\n",
    "            file_path = os.path.join(subdir, file)\n",
    "            temp_df = pd.read_csv(file_path)\n",
    "            if temp_df.shape[0] == 0:  # only header\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted empty file: {file_path}\")\n",
    "\n",
    "print(\"Empty CSV files removed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b5fca0-0bc2-44ca-9c61-37fb2caf10bd",
   "metadata": {},
   "source": [
    "# CHECK 4: Duplicated races - overall + Saturday or other day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba836d6-5c79-479e-9ee0-1e7ea762abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to check for duplicates (ignoring Division)\n",
    "subset_cols = ['Race', 'Gender', 'Name', 'Nation', 'Age Group', 'Total Time']\n",
    "\n",
    "# Set display options for wide output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 2000)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Show duplicate rows in DOUBLES based on subset\n",
    "duplicates_doubles_rows = DOUBLES_df[DOUBLES_df.duplicated(subset=subset_cols, keep=False)]\n",
    "print(f\"DOUBLES dataset duplicate rows based on {subset_cols} ({len(duplicates_doubles_rows)} total):\")\n",
    "print(duplicates_doubles_rows)\n",
    "\n",
    "# Show duplicate rows in HYROX_pro based on subset\n",
    "duplicates_pro_rows = HYROX_pro_df[HYROX_pro_df.duplicated(subset=subset_cols, keep=False)]\n",
    "print(f\"HYROX_pro dataset duplicate rows based on {subset_cols} ({len(duplicates_pro_rows)} total):\")\n",
    "print(duplicates_pro_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086f6743-089e-4988-86e0-cb09bdaeedd9",
   "metadata": {},
   "source": [
    "# CHECK 5: Missing genders in races. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6391fc-51c3-4054-8a31-162d2a4e09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check missing genders per Race+Division\n",
    "def check_missing_genders(df, expected_genders):\n",
    "    missing_info = {}\n",
    "    for (race, division), group in df.groupby(['Race', 'Division']):\n",
    "        present_genders = set(group['Gender'].unique())\n",
    "        missing = set(expected_genders) - present_genders\n",
    "        if missing:\n",
    "            missing_info[(race, division)] = missing\n",
    "    return missing_info\n",
    "\n",
    "# DOUBLES dataset (expected genders: Men, Women, Mixed)\n",
    "doubles_missing_genders = check_missing_genders(DOUBLES_df, ['Men', 'Women', 'Mixed'])\n",
    "print(\"DOUBLES missing genders per Race+Division:\")\n",
    "print(doubles_missing_genders)\n",
    "\n",
    "# HYROX_pro dataset (expected genders: Men, Women)\n",
    "pro_missing_genders = check_missing_genders(HYROX_pro_df, ['Men', 'Women'])\n",
    "print(\"\\nHYROX_pro missing genders per Race+Division:\")\n",
    "print(pro_missing_genders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ba7fe-fff8-47b2-847e-788f5b8c4c98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
